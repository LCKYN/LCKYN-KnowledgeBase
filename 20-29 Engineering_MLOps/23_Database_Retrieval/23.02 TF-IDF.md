---
id: 23.02
tags: [type/concept, status/growing, context/data-engineering, context/llm]
---

# TF-IDF (Term Frequency-Inverse Document Frequency)

## Overview
Statistical measure evaluating word importance in a document relative to a corpus. Foundational for search engines, keyword extraction, and information retrieval.

**Key Idea**: Important words appear frequently in a document but rarely across the corpus.

## Mathematical Formulation

### Term Frequency (TF)
| Variant | Formula | Use Case |
|---------|---------|----------|
| Raw Count | $f_{t,d}$ | Simple counting |
| Normalized | $\frac{f_{t,d}}{\max_{t' \in d} f_{t',d}}$ | Compare across docs |
| Log | $\log(1 + f_{t,d})$ | Reduce high-freq impact |
| Boolean | $1$ if present, $0$ otherwise | Presence/absence |

### Inverse Document Frequency (IDF)
$$\text{IDF}(t, D) = \log \frac{N}{1 + |\{d \in D : t \in d\}|} + 1$$

Where $N$ = total docs, denominator = docs containing term $t$

### Combined Score
$$\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)$$

## Scikit-learn Usage

Key parameters:
- `max_features` - Limit vocabulary size
- `min_df` / `max_df` - Filter rare/common terms
- `ngram_range` - Include phrases (e.g., `(1, 2)` for bigrams)
- `stop_words` - Remove common words
- `sublinear_tf` - Use log(1 + tf)

Typical workflow: `fit_transform()` on corpus, `transform()` on queries, then `cosine_similarity()` for comparison.

## BM25 (Enhanced TF-IDF)

Better ranking function with document length normalization:

$$\text{BM25} = \sum_{t \in q} \text{IDF}(t) \cdot \frac{f_{t,d} \cdot (k_1 + 1)}{f_{t,d} + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}$$

| Parameter | Default | Purpose |
|-----------|---------|---------|
| $k_1$ | 1.2-2.0 | Term frequency saturation |
| $b$ | 0.75 | Length normalization strength |

> [!TIP]
> BM25 is used by Elasticsearch and often outperforms vanilla TF-IDF for ranking.

## TF-IDF vs Dense Embeddings

| Aspect | TF-IDF | Embeddings |
|--------|--------|------------|
| Representation | Sparse (vocab size) | Dense (fixed dims) |
| Semantic understanding | None | Strong |
| Exact matching | Excellent | Poor |
| Synonyms | Not handled | Handled |
| Compute cost | Low | Higher |
| Interpretability | High | Low |
| Best for | Keyword search | Semantic search |

## Hybrid Approach

Combine TF-IDF (exact match) + embeddings (semantic):

```
hybrid_score = alpha * tfidf_score + (1 - alpha) * embedding_score
```

**Common Strategy**: Use embeddings for initial retrieval, TF-IDF for re-ranking.

## Use Cases

- **Search ranking** - Score documents against query
- **Keyword extraction** - Find distinctive terms per document
- **Document similarity** - Cosine similarity on TF-IDF vectors
- **Text classification** - Feature vectors for ML models
- **Duplicate detection** - High similarity = potential duplicates

## When to Use

✅ **Good fit**: Exact keyword matching, interpretability needed, small corpus, resource-constrained, baseline system

❌ **Poor fit**: Semantic search, synonyms matter, multilingual, noisy/typo-heavy text

## Related Concepts
- [[23.01 Vector Databases]]
- [[11.12 RAG]]

## References
- Spärck Jones (1972) - Original IDF concept
- Robertson & Walker (1994) - BM25
- Scikit-learn TfidfVectorizer documentation
