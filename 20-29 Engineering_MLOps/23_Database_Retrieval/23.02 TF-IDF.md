---
id: 23.02
tags: [type/concept, status/growing, context/data-engineering, context/llm]
---

# TF-IDF (Term Frequency-Inverse Document Frequency)

## Overview
TF-IDF is a classical statistical measure used to evaluate the importance of a word in a document relative to a collection of documents (corpus). It balances how frequently a term appears in a document against how rare it is across all documents, making it foundational for information retrieval, search engines, and keyword extraction.

## Core Concept

**Key Idea**: Important words appear frequently in a document but rarely across the corpus.

- **High TF-IDF**: Term is frequent in document, rare in corpus → Distinctive/Important
- **Low TF-IDF**: Term is either rare in document OR common across corpus → Less distinctive

## Mathematical Formulation

### Term Frequency (TF)
Measures how often a term appears in a document.

#### Raw Count
$$\text{TF}(t, d) = f_{t,d}$$
Where $f_{t,d}$ is the raw count of term $t$ in document $d$.

#### Normalized (Relative Frequency)
$$\text{TF}(t, d) = \frac{f_{t,d}}{\max_{t' \in d} f_{t',d}}$$
Normalizes by the most frequent term in the document.

#### Log Normalization
$$\text{TF}(t, d) = \log(1 + f_{t,d})$$
Diminishes the impact of very high frequencies.

#### Boolean
$$\text{TF}(t, d) = \begin{cases} 1 & \text{if } f_{t,d} > 0 \\ 0 & \text{otherwise} \end{cases}$$
Binary presence/absence indicator.

### Inverse Document Frequency (IDF)
Measures how rare a term is across the corpus.

#### Standard IDF
$$\text{IDF}(t, D) = \log \frac{N}{|\{d \in D : t \in d\}|}$$

Where:
- $N$ = Total number of documents
- $|\{d \in D : t \in d\}|$ = Number of documents containing term $t$

#### Smooth IDF (prevents division by zero)
$$\text{IDF}(t, D) = \log \frac{N}{1 + |\{d \in D : t \in d\}|} + 1$$

### TF-IDF Score
$$\text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)$$

## Implementation

### From Scratch (Python)
```python
import math
from collections import Counter
from typing import List, Dict

class TfidfVectorizer:
    def __init__(self):
        self.idf_scores = {}
        self.vocabulary = set()

    def fit(self, documents: List[str]):
        """Calculate IDF scores from corpus."""
        n_docs = len(documents)
        doc_term_freq = Counter()

        # Count documents containing each term
        for doc in documents:
            terms = set(doc.lower().split())
            self.vocabulary.update(terms)
            for term in terms:
                doc_term_freq[term] += 1

        # Calculate IDF for each term
        for term in self.vocabulary:
            self.idf_scores[term] = math.log(
                n_docs / (1 + doc_term_freq[term])
            ) + 1

    def transform(self, documents: List[str]) -> List[Dict[str, float]]:
        """Convert documents to TF-IDF vectors."""
        tfidf_vectors = []

        for doc in documents:
            terms = doc.lower().split()
            term_freq = Counter(terms)
            max_freq = max(term_freq.values()) if term_freq else 1

            # Calculate TF-IDF for each term
            tfidf_vec = {}
            for term, freq in term_freq.items():
                if term in self.idf_scores:
                    tf = freq / max_freq  # Normalized TF
                    idf = self.idf_scores[term]
                    tfidf_vec[term] = tf * idf

            tfidf_vectors.append(tfidf_vec)

        return tfidf_vectors

    def fit_transform(self, documents: List[str]) -> List[Dict[str, float]]:
        """Fit and transform in one step."""
        self.fit(documents)
        return self.transform(documents)

# Example usage
corpus = [
    "the cat sat on the mat",
    "the dog sat on the log",
    "cats and dogs are enemies"
]

vectorizer = TfidfVectorizer()
tfidf_matrix = vectorizer.fit_transform(corpus)

for i, vec in enumerate(tfidf_matrix):
    print(f"Doc {i}: {vec}")
```

### Using Scikit-learn
```python
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Initialize vectorizer
vectorizer = TfidfVectorizer(
    max_features=1000,      # Limit vocabulary size
    min_df=2,               # Ignore terms in < 2 documents
    max_df=0.8,             # Ignore terms in > 80% of documents
    ngram_range=(1, 2),     # Unigrams and bigrams
    stop_words='english',   # Remove common words
    sublinear_tf=True,      # Use log(1 + tf) instead of tf
    smooth_idf=True         # Add 1 to document frequencies
)

# Fit and transform
tfidf_matrix = vectorizer.fit_transform(corpus)

# Get feature names (vocabulary)
feature_names = vectorizer.get_feature_names_out()

# Convert to array (dense)
tfidf_array = tfidf_matrix.toarray()

print(f"Shape: {tfidf_array.shape}")  # (n_documents, n_features)
print(f"Vocabulary size: {len(feature_names)}")
```

### Dense Vector Representation
```python
# Get TF-IDF scores as dense vectors
doc_vectors = tfidf_matrix.toarray()

# Shape: (n_documents, n_vocabulary)
print(doc_vectors.shape)  # e.g., (1000, 5000)

# Access specific document vector
doc_0_vector = doc_vectors[0]

# Get top N terms for a document
def get_top_terms(doc_idx, n=10):
    doc_vec = doc_vectors[doc_idx]
    top_indices = np.argsort(doc_vec)[-n:][::-1]

    return [
        (feature_names[i], doc_vec[i])
        for i in top_indices
        if doc_vec[i] > 0
    ]

# Get top terms for document 0
top_terms = get_top_terms(0, n=5)
print(top_terms)
```

## Use Cases

### 1. Document Similarity
```python
from sklearn.metrics.pairwise import cosine_similarity

# Calculate similarity between documents
similarity_matrix = cosine_similarity(tfidf_matrix)

def find_similar_docs(doc_idx, top_k=5):
    """Find most similar documents."""
    similarities = similarity_matrix[doc_idx]
    # Exclude self (similarity = 1.0)
    similar_indices = np.argsort(similarities)[-top_k-1:-1][::-1]

    return [
        (idx, similarities[idx])
        for idx in similar_indices
    ]

similar = find_similar_docs(0, top_k=3)
print(f"Most similar to doc 0: {similar}")
```

### 2. Keyword Extraction
```python
def extract_keywords(document, top_n=10):
    """Extract top keywords from a document."""
    # Transform single document
    vec = vectorizer.transform([document])

    # Get feature names and scores
    feature_array = np.array(vectorizer.get_feature_names_out())
    tfidf_sorting = np.argsort(vec.toarray()).flatten()[::-1]

    # Get top N keywords
    top_keywords = feature_array[tfidf_sorting][:top_n]
    top_scores = vec.toarray().flatten()[tfidf_sorting][:top_n]

    return list(zip(top_keywords, top_scores))

keywords = extract_keywords("Machine learning and artificial intelligence")
print(keywords)
```

### 3. Search Engine Ranking
```python
def search(query, documents, top_k=5):
    """Simple search using TF-IDF similarity."""
    # Fit vectorizer on documents
    vectorizer = TfidfVectorizer()
    doc_vectors = vectorizer.fit_transform(documents)

    # Transform query
    query_vector = vectorizer.transform([query])

    # Calculate similarity
    similarities = cosine_similarity(query_vector, doc_vectors).flatten()

    # Get top results
    top_indices = np.argsort(similarities)[-top_k:][::-1]

    results = [
        {
            'doc_idx': idx,
            'score': similarities[idx],
            'text': documents[idx][:100]  # Preview
        }
        for idx in top_indices
        if similarities[idx] > 0
    ]

    return results

# Search example
docs = [
    "Python machine learning tutorial",
    "JavaScript web development guide",
    "Machine learning with Python and scikit-learn",
    "React and JavaScript frameworks"
]

results = search("python machine learning", docs, top_k=3)
for r in results:
    print(f"Score: {r['score']:.3f} - {r['text']}")
```

### 4. Document Classification
```python
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

# Prepare data
X_train, X_test, y_train, y_test = train_test_split(
    tfidf_matrix, labels, test_size=0.2
)

# Train classifier
classifier = LogisticRegression(max_iter=1000)
classifier.fit(X_train, y_train)

# Predict
predictions = classifier.predict(X_test)

# Accuracy
accuracy = (predictions == y_test).mean()
print(f"Accuracy: {accuracy:.3f}")
```

## Advanced Techniques

### BM25 (Best Matching 25)
Enhanced TF-IDF with document length normalization:

$$\text{BM25}(q, d) = \sum_{t \in q} \text{IDF}(t) \cdot \frac{f_{t,d} \cdot (k_1 + 1)}{f_{t,d} + k_1 \cdot (1 - b + b \cdot \frac{|d|}{\text{avgdl}})}$$

Where:
- $k_1$ = Term frequency saturation parameter (typically 1.2-2.0)
- $b$ = Length normalization parameter (typically 0.75)
- $|d|$ = Document length
- $\text{avgdl}$ = Average document length in corpus

```python
from rank_bm25 import BM25Okapi

# Tokenize corpus
tokenized_corpus = [doc.lower().split() for doc in corpus]

# Initialize BM25
bm25 = BM25Okapi(tokenized_corpus)

# Search
query = "cat sat mat"
tokenized_query = query.split()
doc_scores = bm25.get_scores(tokenized_query)

# Get top documents
top_docs = np.argsort(doc_scores)[::-1][:5]
print(f"Top docs: {top_docs}")
```

### N-grams
Capture multi-word phrases:
```python
vectorizer = TfidfVectorizer(
    ngram_range=(1, 3),  # Unigrams, bigrams, trigrams
    max_features=10000
)

# Now captures: "machine", "machine learning", "machine learning model"
tfidf_matrix = vectorizer.fit_transform(corpus)
```

### Character N-grams
Useful for handling typos and morphological variations:
```python
vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5)  # Character 3-grams to 5-grams
)

# Captures: "mach", "achi", "chin", etc.
```

## Hybrid Approaches with Embeddings

### TF-IDF + Dense Embeddings
Combine keyword matching with semantic similarity:

```python
from sklearn.preprocessing import normalize

def hybrid_search(query, documents, alpha=0.5):
    """
    Hybrid search combining TF-IDF and embeddings.

    alpha: Weight for TF-IDF (1-alpha for embeddings)
    """
    # TF-IDF search
    tfidf_vec = TfidfVectorizer()
    doc_tfidf = tfidf_vec.fit_transform(documents)
    query_tfidf = tfidf_vec.transform([query])
    tfidf_scores = cosine_similarity(query_tfidf, doc_tfidf).flatten()

    # Embedding search
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')

    doc_embeddings = model.encode(documents)
    query_embedding = model.encode([query])
    embedding_scores = cosine_similarity(query_embedding, doc_embeddings).flatten()

    # Normalize scores to [0, 1]
    tfidf_scores = (tfidf_scores - tfidf_scores.min()) / (tfidf_scores.max() - tfidf_scores.min() + 1e-10)
    embedding_scores = (embedding_scores - embedding_scores.min()) / (embedding_scores.max() - embedding_scores.min() + 1e-10)

    # Combine scores
    hybrid_scores = alpha * tfidf_scores + (1 - alpha) * embedding_scores

    # Rank results
    top_indices = np.argsort(hybrid_scores)[::-1]

    return [
        {
            'doc_idx': idx,
            'hybrid_score': hybrid_scores[idx],
            'tfidf_score': tfidf_scores[idx],
            'embedding_score': embedding_scores[idx],
            'text': documents[idx]
        }
        for idx in top_indices[:5]
    ]
```

### Re-ranking with TF-IDF
Use embeddings for initial retrieval, TF-IDF for re-ranking:

```python
def two_stage_search(query, documents, initial_k=20, final_k=5):
    """
    Stage 1: Fast embedding search for candidates
    Stage 2: Re-rank with TF-IDF for exact matching
    """
    # Stage 1: Embedding search (semantic)
    from sentence_transformers import SentenceTransformer
    model = SentenceTransformer('all-MiniLM-L6-v2')

    doc_embeddings = model.encode(documents)
    query_embedding = model.encode([query])

    similarities = cosine_similarity(query_embedding, doc_embeddings).flatten()
    candidate_indices = np.argsort(similarities)[-initial_k:][::-1]

    # Stage 2: TF-IDF re-ranking
    candidates = [documents[i] for i in candidate_indices]

    vectorizer = TfidfVectorizer()
    candidate_tfidf = vectorizer.fit_transform(candidates)
    query_tfidf = vectorizer.transform([query])

    tfidf_scores = cosine_similarity(query_tfidf, candidate_tfidf).flatten()

    # Get final top K
    final_indices = np.argsort(tfidf_scores)[-final_k:][::-1]

    return [candidate_indices[i] for i in final_indices]
```

## TF-IDF vs Modern Approaches

| Aspect | TF-IDF | Dense Embeddings |
|--------|--------|------------------|
| **Representation** | Sparse vectors | Dense vectors |
| **Dimensionality** | Vocabulary size (10K-100K+) | Fixed (384-1536) |
| **Semantic understanding** | None (keyword matching) | Strong (contextual) |
| **Exact matching** | Excellent | Poor |
| **Synonyms** | Not handled | Handled well |
| **Computational cost** | Low | Higher (embedding generation) |
| **Interpretability** | High (see exact terms) | Low (black box) |
| **Training required** | No | Yes (pre-trained models) |
| **Best for** | Keyword search, exact terms | Semantic search, concept matching |

## When to Use TF-IDF

### ✅ Good Use Cases
1. **Exact keyword matching**: Legal documents, code search
2. **Interpretability needed**: Show why documents matched
3. **Small corpus**: < 10K documents
4. **Resource-constrained**: Low memory/compute
5. **Baseline system**: Quick prototype or benchmark
6. **Keyword extraction**: Identify important terms
7. **Sparse data**: When most features are zeros

### ❌ When to Avoid
1. **Semantic search**: "car" won't match "automobile"
2. **Multilingual**: No cross-language matching
3. **Short queries**: Single-word queries lack context
4. **Domain mismatch**: Corpus and query vocabulary differ
5. **Noisy text**: Typos and variations hurt performance

## Optimization Tips

### Memory Efficiency
```python
# Use sparse matrices (don't convert to dense)
from scipy.sparse import csr_matrix

tfidf_matrix = vectorizer.fit_transform(corpus)  # Sparse by default
# Don't do: tfidf_matrix.toarray()  # Converts to dense!

# Save sparse matrix
from scipy.sparse import save_npz, load_npz

save_npz('tfidf_matrix.npz', tfidf_matrix)
loaded_matrix = load_npz('tfidf_matrix.npz')
```

### Speed Optimization
```python
# Limit vocabulary size
vectorizer = TfidfVectorizer(
    max_features=5000,      # Keep top 5000 terms by frequency
    min_df=5,               # Ignore rare terms (< 5 docs)
    max_df=0.8              # Ignore common terms (> 80% docs)
)

# Use binary instead of TF-IDF for very large corpora
vectorizer = TfidfVectorizer(binary=True)
```

### Incremental Learning
```python
from sklearn.feature_extraction.text import HashingVectorizer

# Doesn't store vocabulary (constant memory)
vectorizer = HashingVectorizer(
    n_features=2**16,  # Hash space size
    alternate_sign=False
)

# Can process in batches
for batch in document_batches:
    batch_vectors = vectorizer.transform(batch)
    # Process batch
```

## Production Considerations

### Persistence
```python
import pickle

# Save vectorizer
with open('tfidf_vectorizer.pkl', 'wb') as f:
    pickle.dump(vectorizer, f)

# Load vectorizer
with open('tfidf_vectorizer.pkl', 'rb') as f:
    vectorizer = pickle.load(f)

# Use with new documents
new_vectors = vectorizer.transform(new_documents)
```

### Updating with New Documents
```python
# Option 1: Retrain periodically
# Refit vectorizer with all documents (old + new)

# Option 2: Approximate update (HashingVectorizer)
hasher = HashingVectorizer(n_features=2**16)
# No retraining needed for new documents

# Option 3: Incremental IDF update
def update_idf(old_idf, old_n, new_docs):
    """Approximate IDF update without full retraining."""
    # Update document counts and recalculate IDF
    # Useful for streaming scenarios
    pass
```

## Integration Examples

### Elasticsearch (BM25)
```python
from elasticsearch import Elasticsearch

es = Elasticsearch(['http://localhost:9200'])

# Index documents
for i, doc in enumerate(documents):
    es.index(
        index='my_index',
        id=i,
        body={'text': doc}
    )

# Search (uses BM25 by default)
results = es.search(
    index='my_index',
    body={
        'query': {
            'match': {
                'text': 'machine learning'
            }
        }
    }
)
```

### Whoosh (Pure Python)
```python
from whoosh import index
from whoosh.fields import Schema, TEXT, ID
from whoosh.qparser import QueryParser

# Define schema
schema = Schema(
    id=ID(stored=True),
    content=TEXT(stored=True)
)

# Create index
ix = index.create_in("indexdir", schema)

# Add documents
writer = ix.writer()
for i, doc in enumerate(documents):
    writer.add_document(id=str(i), content=doc)
writer.commit()

# Search
with ix.searcher() as searcher:
    query = QueryParser("content", ix.schema).parse("machine learning")
    results = searcher.search(query)
    for r in results:
        print(r['content'])
```

## Best Practices

1. **Preprocessing**: Lowercase, remove punctuation, stem/lemmatize
2. **Stop words**: Remove common words unless phrase search is important
3. **Limit vocabulary**: Use min_df/max_df to filter noise
4. **Normalization**: Use L2 normalization for similarity calculations
5. **Tune parameters**: Experiment with TF/IDF variants
6. **Consider alternatives**: BM25 often outperforms vanilla TF-IDF
7. **Hybrid approach**: Combine with embeddings for best results
8. **Monitor performance**: Track search quality metrics

## Related Concepts
- [[23.01 Vector Databases]]
- [[11.12 RAG]]
- [[41.01 Parquet File Format]]

## References
- "A Statistical Interpretation of Term Specificity and Its Application in Retrieval" (Spärck Jones, 1972)
- "Some Simple Effective Approximations to the 2-Poisson Model for Probabilistic Weighted Retrieval" (Robertson & Walker, 1994) - BM25
- Scikit-learn Documentation: TfidfVectorizer
- "Introduction to Information Retrieval" (Manning, Raghavan & Schütze)
