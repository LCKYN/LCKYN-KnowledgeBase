---
id: 32.05
tags: [type/concept, status/growing, context/finops, context/mle]
---

# Training vs Inference Costs

## Overview
Training and inference have fundamentally different cost profiles. Training is periodic compute-intensive investment; inference is ongoing operational cost scaling with usage.

## Cost Profile Comparison

| Aspect | Training | Inference |
|--------|----------|-----------|
| **Frequency** | Periodic (weekly/monthly) | Continuous |
| **Duration** | Hours to weeks | Milliseconds per request |
| **GPU Type** | High-memory (A100, H100) | Can use smaller GPUs |
| **Scaling** | Data size, model params | Request volume |
| **Optimization** | Spot instances, checkpointing | Batching, caching, quantization |
| **Cost Pattern** | Predictable bursts | Variable, usage-based |

## Training Costs

### Cost Components

| Component | Description |
|-----------|-------------|
| **Compute** | GPU hours, CPU preprocessing |
| **Storage** | Dataset, checkpoints, artifacts |
| **Transfer** | Data loading, multi-node sync |
| **Tooling** | Experiment tracking, orchestration |

### GPU Pricing (AWS SageMaker, Jan 2026)

| GPU | Memory | On-Demand/hr | Spot/hr | FP16 TFLOPs |
|-----|--------|--------------|---------|-------------|
| T4 | 16GB | $0.53 | $0.16 | 65 |
| L4 | 24GB | $0.81 | $0.24 | 121 |
| A10G | 24GB | $1.21 | $0.36 | 125 |
| A100 40GB | 40GB | $4.10 | $1.23 | 312 |
| A100 80GB | 80GB | $5.12 | $1.54 | 312 |
| H100 80GB | 80GB | $8.22 | $2.47 | 989 |

### Training Cost by Scale

| Model Size | Typical Cost | GPUs | Duration |
|------------|--------------|------|----------|
| < 1B params | $100-1K | Single GPU | Hours-days |
| 1-10B params | $1K-10K | 4-8 GPUs | Days-weeks |
| 10-100B params | $10K-1M | 100+ GPUs | Weeks-months |
| 100B+ params | $1M-100M+ | 1000s GPUs | Months |

### Training Optimizations

| Strategy | Savings | Tradeoff |
|----------|---------|----------|
| Spot Instances | 70% | Interruptions, need checkpointing |
| Mixed Precision (FP16/BF16) | 50% | Minor accuracy loss possible |
| Gradient Checkpointing | 30% | 10-20% slower |
| LoRA/QLoRA | 90% | Only trains adapters |
| Knowledge Distillation | 80% | Needs teacher, quality loss |

## Inference Costs

### Cost Components

| Component | Description |
|-----------|-------------|
| **Compute** | GPU/CPU time per request |
| **Serving** | Load balancing, health checks |
| **Scaling** | Auto-scaling, cold starts |
| **Latency SLA** | Redundancy, caching infra |

### Per-Request Cost Factors

- GPU time (batch size affects efficiency)
- Memory cost (model in GPU memory)
- Overhead (~20%): monitoring, load balancing

### API vs Self-Hosted

| Factor | API | Self-Hosted |
|--------|-----|-------------|
| Infrastructure | None | Required |
| Scaling | Automatic | Manual/auto-scaling |
| Latest models | Immediate | Deployment lag |
| Per-token cost | Higher | Lower at scale |
| Data privacy | Shared | Full control |
| Lock-in | Vendor | None |

**Break-even**: Typically 100K-1M requests/month, depending on model and infrastructure costs.

### Inference Optimizations

| Strategy | Savings | Latency Impact |
|----------|---------|----------------|
| Request Batching | 30-50% | +50-100ms |
| KV Cache | 40-60% | Improves |
| Prompt Caching | 50-90% | Improves |
| Quantization (INT8/4) | 50-75% | Improves |
| Speculative Decoding | 20-40% | Improves |
| Continuous Batching | 30-50% | Improves |

## Decision Framework

### When to Train

| Scenario | Recommendation |
|----------|----------------|
| Custom domain data | Fine-tune |
| Data privacy critical | Self-host + train |
| High volume (>100K/mo) | Consider training |
| Unique requirements | Custom model |
| Stable, known problem | Invest in training |

### When to Use API

| Scenario | Recommendation |
|----------|----------------|
| < 10K requests/month | API |
| Rapid experimentation | API |
| Latest models needed | API |
| Variable/unpredictable load | API |
| Quick time-to-market | API |

### Hybrid Approach

- **API**: Development, low-volume features, burst capacity
- **Self-hosted**: High-volume, stable features, privacy-sensitive

## Cost Allocation Model

For annual planning:

**Training** (periodic):
- Initial training cost
- Retraining frequency × retraining cost

**Inference** (ongoing):
- Requests/month × cost/request × 12

**Amortized cost/request** = Total annual / Annual requests

> [!TIP]
> For fine-tuned models, training is often <20% of total annual cost when serving at scale.

## Monitoring Metrics

### Training
- Cost per training run
- GPU utilization (MFU)
- Cost per epoch
- Cost per billion parameters

### Inference
- Cost per request
- Cost per 1K tokens
- Requests per dollar
- Cost by feature/user

## Related Concepts
- [[32_Cost_Management_MOC]] - Parent category
- [[32.04 Multi-Model Cost Strategy]] - Model selection based on costs
- [[32.01 Cost Monitoring Tools]] - How to track GPU and API costs
- [[32.06 FinOps Practices]] - Optimization strategies for both phases

## References
- Scaling Laws (Chinchilla, Kaplan)
- Cloud provider GPU pricing
- vLLM, TensorRT-LLM documentation
