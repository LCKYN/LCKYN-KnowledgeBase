---
id: 71.05
tags: [type/concept, status/evergreen, context/dev-tools]
---

# Programmer Performance Micro-Techniques

## Overview
Small but impactful code-level optimizations â€” applicable regardless of language. Useful when profiling reveals hotspots in tight loops or frequently called functions.

### Runtime Applicability Legend
| Label | Meaning |
|---|---|
| ğŸŒ Universal | Works in both interpreted and compiled languages |
| âš™ï¸ Compiled | Compiler may apply automatically; most impactful in native code |
| ğŸ”„ Manual (interpreted) | Compiler handles this automatically â€” must be done by hand in interpreted languages (Python, Ruby, JS) |

---

## 1. Short-Circuit Evaluation `ğŸŒ Universal`

In most languages, `and` / `or` (or `&&` / `||`) **stop evaluating as soon as the result is determined**.

- `A and B` â†’ if `A` is falsy, `B` is **never evaluated**
- `A or B` â†’ if `A` is truthy, `B` is **never evaluated**

```python
# Put cheap/likely-false condition first in `and`
if is_valid(x) and expensive_check(x):  # expensive_check skipped if invalid

# Put cheap/likely-true condition first in `or`
result = cache.get(key) or compute(key)
```

> [!TIP] Ordering matters
> Place the **cheapest** or **most likely to short-circuit** condition first to avoid unnecessary work.

**Common patterns:**
- `x is not None and x > 0` â€” guard before comparison
- `value or default` â€” fallback without `if/else`
- `list and list[0]` â€” safe first-element access

---

## 2. Memoization `ğŸŒ Universal`

Cache the result of a function call so repeated calls with the same arguments return instantly.

### `functools.lru_cache` (built-in)
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def fib(n: int) -> int:
    if n < 2:
        return n
    return fib(n - 1) + fib(n - 2)
```

### `functools.cache` (Python 3.9+ â€” unbounded)
```python
from functools import cache

@cache
def get_config(env: str) -> dict:
    return load_from_disk(env)
```

| Decorator | Bound | Thread-safe | Use when |
|---|---|---|---|
| `@lru_cache(maxsize=N)` | Fixed | âœ… | Limited memory budget |
| `@cache` | Unbounded | âœ… | Inputs space is small/known |
| Manual `dict` cache | Custom | âŒ | Need fine-grained control |

> [!WARNING] Only pure functions
> Memoization assumes same inputs â†’ same output. **Don't cache functions with side effects or mutable arguments.**

---

## 3. Loop Invariant Code Motion `âš™ï¸ Compiled (auto)` `ğŸ”„ Manual (interpreted)`

Move computations that **don't change between iterations** outside the loop.

> [!INFO] Compiler note
> C/C++/Rust/Java (JIT) compilers apply **LICM (Loop Invariant Code Motion)** automatically as an optimization pass. In Python, Ruby, or JavaScript (non-JIT), **you must hoist manually**.

```python
# Bad â€” len(items) and threshold recalculated every iteration
for i in range(len(items)):
    if items[i] > config.threshold * 1.5:
        process(items[i])

# Good â€” hoist invariants out
n = len(items)
limit = config.threshold * 1.5
for i in range(n):
    if items[i] > limit:
        process(items[i])
```

**Common invariants to hoist:**
- `len(collection)` â€” doesn't change if you're not mutating it
- Method lookups: `append = result.append` (avoids attribute lookup each iteration)
- Constant expressions: `math.sqrt(2)`, threshold multiplications
- Regex compilation: `re.compile(pattern)` outside loop

```python
# Method lookup hoisting (high-frequency loops)
result = []
append = result.append      # hoist method lookup
for x in large_iterable:
    append(transform(x))    # ~15% faster than result.append(...)
```

---

## 4. Branch Prediction Optimization `âš™ï¸ Compiled (most impactful)` `ğŸŒ Algorithmic level`

CPUs have a hardware **branch predictor** â€” it guesses which path an `if` will take and pre-fetches instructions. Mispredictions flush the pipeline and stall execution.

> [!INFO] Compiler note
> In compiled languages (C/C++, Rust), you can give the compiler hints:
> - **GCC/Clang**: `__builtin_expect(cond, 1)` for likely, `__builtin_expect(cond, 0)` for unlikely
> - **C++20**: `[[likely]]` / `[[unlikely]]` attributes
> - **Rust**: `std::hint::likely()` / `unlikely()` (nightly)
>
> In interpreted languages (Python, Ruby), interpreter overhead dominates â€” focus on **algorithmic branch reduction** instead.

### Sort data before processing
```python
# Unpredictable branching
for x in random_data:
    if x > threshold:
        process(x)

# Better â€” filter first, then process (predictable path)
hot = [x for x in data if x > threshold]
for x in hot:
    process(x)
```

### Avoid branches in tight loops â€” use vectorized ops
```python
import numpy as np

# Branchy Python loop
result = [x * 2 if x > 0 else 0 for x in data]

# Branchless numpy
arr = np.array(data)
result = np.where(arr > 0, arr * 2, 0)
```

### Early return / guard clauses
Flatten nested `if` chains â€” reduces cognitive load and helps the interpreter:
```python
# Nested (bad)
def process(x):
    if x is not None:
        if x > 0:
            return x * 2

# Guard clauses (good)
def process(x):
    if x is None: return None
    if x <= 0:    return None
    return x * 2
```

> [!TIP] Profile first
> These optimizations matter only in hot paths. Use `cProfile` or `line_profiler` to confirm a bottleneck before optimizing.

---

## Summary

| Technique | Applicability | What it avoids | Best for |
|---|---|---|---|
| Short-circuit | ğŸŒ Universal | Unnecessary evaluation | Boolean chains, guard conditions |
| Memoization | ğŸŒ Universal | Redundant computation | Recursive funcs, repeated lookups |
| Loop invariant hoisting | âš™ï¸ Auto / ğŸ”„ Manual | Per-iteration redundant work | Tight loops with constant subexpressions |
| Branch reduction | âš™ï¸ Compiled + hints / ğŸŒ Algorithmic | Pipeline stalls & branch overhead | Systems code, high-throughput pipelines |

## Related Concepts
- [[71_Python_Snippets_MOC]] - Parent category
- [[71.04 Google Python Style Guide]] - Code clarity trumps micro-optimization
