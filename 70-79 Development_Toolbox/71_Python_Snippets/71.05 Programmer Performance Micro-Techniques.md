---
id: 71.05
tags: [type/concept, status/evergreen, context/dev-tools]
---

# Programmer Performance Micro-Techniques

## Overview
Small but impactful code-level optimizations ‚Äî applicable regardless of language. Useful when profiling reveals hotspots in tight loops or frequently called functions.

### Runtime Applicability Legend
| Label | Meaning |
|---|---|
| üåê Universal | Works in both interpreted and compiled languages |
| ‚öôÔ∏è Compiled | Compiler may apply automatically; most impactful in native code |
| üîÑ Manual (interpreted) | Compiler handles this automatically ‚Äî must be done by hand in interpreted languages (Python, Ruby, JS) |

---

## 1. Short-Circuit Evaluation `üåê Universal`

In most languages, `and` / `or` (or `&&` / `||`) **stop evaluating as soon as the result is determined**.

- `A and B` ‚Üí if `A` is falsy, `B` is **never evaluated**
- `A or B` ‚Üí if `A` is truthy, `B` is **never evaluated**

```python
# Put cheap/likely-false condition first in `and`
if is_valid(x) and expensive_check(x):  # expensive_check skipped if invalid

# Put cheap/likely-true condition first in `or`
result = cache.get(key) or compute(key)
```

> [!TIP] Ordering matters
> Place the **cheapest** or **most likely to short-circuit** condition first to avoid unnecessary work.

**Common patterns:**
- `x is not None and x > 0` ‚Äî guard before comparison
- `value or default` ‚Äî fallback without `if/else`
- `list and list[0]` ‚Äî safe first-element access

---

## 2. Memoization `üåê Universal`

Cache the result of a function call so repeated calls with the same arguments return instantly.

### `functools.lru_cache` (built-in)
```python
from functools import lru_cache

@lru_cache(maxsize=128)
def fib(n: int) -> int:
    if n < 2:
        return n
    return fib(n - 1) + fib(n - 2)
```

### `functools.cache` (Python 3.9+ ‚Äî unbounded)
```python
from functools import cache

@cache
def get_config(env: str) -> dict:
    return load_from_disk(env)
```

| Decorator | Bound | Thread-safe | Use when |
|---|---|---|---|
| `@lru_cache(maxsize=N)` | Fixed | ‚úÖ | Limited memory budget |
| `@cache` | Unbounded | ‚úÖ | Inputs space is small/known |
| Manual `dict` cache | Custom | ‚ùå | Need fine-grained control |

> [!WARNING] Only pure functions
> Memoization assumes same inputs ‚Üí same output. **Don't cache functions with side effects or mutable arguments.**

---

## 3. Loop Invariant Code Motion `‚öôÔ∏è Compiled (auto)` `üîÑ Manual (interpreted)`

Move computations that **don't change between iterations** outside the loop.

> [!INFO] Compiler note
> C/C++/Rust/Java (JIT) compilers apply **LICM (Loop Invariant Code Motion)** automatically as an optimization pass. In Python, Ruby, or JavaScript (non-JIT), **you must hoist manually**.

```python
# Bad ‚Äî len(items) and threshold recalculated every iteration
for i in range(len(items)):
    if items[i] > config.threshold * 1.5:
        process(items[i])

# Good ‚Äî hoist invariants out
n = len(items)
limit = config.threshold * 1.5
for i in range(n):
    if items[i] > limit:
        process(items[i])
```

**Common invariants to hoist:**
- `len(collection)` ‚Äî doesn't change if you're not mutating it
- Method lookups: `append = result.append` (avoids attribute lookup each iteration)
- Constant expressions: `math.sqrt(2)`, threshold multiplications
- Regex compilation: `re.compile(pattern)` outside loop

```python
# Method lookup hoisting (high-frequency loops)
result = []
append = result.append      # hoist method lookup
for x in large_iterable:
    append(transform(x))    # ~15% faster than result.append(...)
```

---

## 4. Branch Prediction Optimization `‚öôÔ∏è Compiled (most impactful)` `üåê Algorithmic level`

CPUs have a hardware **branch predictor** ‚Äî it guesses which path an `if` will take and pre-fetches instructions. Mispredictions flush the pipeline and stall execution.

> [!INFO] Compiler note
> In compiled languages (C/C++, Rust), you can give the compiler hints:
> - **GCC/Clang**: `__builtin_expect(cond, 1)` for likely, `__builtin_expect(cond, 0)` for unlikely
> - **C++20**: `[[likely]]` / `[[unlikely]]` attributes
> - **Rust**: `std::hint::likely()` / `unlikely()` (nightly)
>
> In interpreted languages (Python, Ruby), interpreter overhead dominates ‚Äî focus on **algorithmic branch reduction** instead.

### Sort data before processing
```python
# Unpredictable branching
for x in random_data:
    if x > threshold:
        process(x)

# Better ‚Äî filter first, then process (predictable path)
hot = [x for x in data if x > threshold]
for x in hot:
    process(x)
```

### Avoid branches in tight loops ‚Äî use vectorized ops
```python
import numpy as np

# Branchy Python loop
result = [x * 2 if x > 0 else 0 for x in data]

# Branchless numpy
arr = np.array(data)
result = np.where(arr > 0, arr * 2, 0)
```

### Early return / guard clauses
Flatten nested `if` chains ‚Äî reduces cognitive load and helps the interpreter:
```python
# Nested (bad)
def process(x):
    if x is not None:
        if x > 0:
            return x * 2

# Guard clauses (good)
def process(x):
    if x is None: return None
    if x <= 0:    return None
    return x * 2
```

> [!TIP] Profile first
> These optimizations matter only in hot paths. Use `cProfile` or `line_profiler` to confirm a bottleneck before optimizing.

---

## 5. Common Subexpression Elimination (CSE) `‚öôÔ∏è Compiled (auto)` `üîÑ Manual (interpreted)`

If the same expression is computed **more than once** with the same operands, compute it once and reuse the result.

> [!INFO] Compiler note
> Compilers (GCC, Clang, Rust/LLVM) perform CSE automatically. In interpreted languages, you must do it manually.

```python
# Bad ‚Äî (a * b) computed twice
result = (a * b) + c / (a * b)

# Good ‚Äî compute once, reuse
ab = a * b
result = ab + c / ab
```

```python
# Also applies to repeated attribute/index access
# Bad
if obj.config.timeout > 0 and obj.config.timeout < 30:
    use(obj.config.timeout)

# Good
t = obj.config.timeout
if 0 < t < 30:
    use(t)
```

---

## 6. Dead Code Elimination (DCE) `‚öôÔ∏è Compiled (auto)` `üîÑ Manual (interpreted)`

Remove code that is **never reachable** or whose result is **never used**. Reduces binary size and removes unnecessary work.

> [!INFO] Compiler note
> Compilers eliminate dead code automatically (unreachable branches, unused variables). In interpreted languages it still executes ‚Äî remove it manually.

```python
# Dead assignment ‚Äî result never used
def compute(x):
    unused = expensive_setup()   # ‚Üê dead, remove this
    return x * 2

# Unreachable branch
DEBUG = False
if DEBUG:                        # ‚Üê always false, dead block
    log_verbose_output()
```

**Common sources of dead code:**
- Feature flags hardcoded to `False`
- Variables assigned but never read
- Functions defined but never called
- `else` branches after `return`/`raise`
- Duplicate logic left after refactoring

> [!TIP] Tools
> - Python: `vulture`, `pyflakes`, `ruff` flag unused variables/imports
> - Rust: compiler warns on unused variables/functions by default
> - C/C++: `-Wunused` flags with GCC/Clang

---

## 7. Function Inlining `‚öôÔ∏è Compiled (auto)` `üîÑ Manual (hot paths only)`

Replace a function **call** with the function's **body** directly at the call site ‚Äî eliminates call overhead (stack frame setup, argument passing, return).

> [!INFO] Compiler note
> Compilers inline small/hot functions automatically. In C/C++ use `inline` hint; in Rust `#[inline]` / `#[inline(always)]`. Python has no inlining ‚Äî manually inline only in tight loops where call overhead is measured.

```c
// C ‚Äî compiler typically inlines this automatically
static inline int square(int x) { return x * x; }

// Rust ‚Äî explicit hint
#[inline(always)]
fn square(x: i32) -> i32 { x * x }
```

```python
# Python ‚Äî function calls are expensive (~100ns each)
# If profiler shows call overhead in a hot loop, manually inline

# Before
def clamp(v, lo, hi):
    return max(lo, min(hi, v))

result = [clamp(x, 0, 255) for x in pixels]

# After (inlined manually ‚Äî only worth it if proven hot)
result = [max(0, min(255, x)) for x in pixels]
```

> [!WARNING] Don't inline for readability
> Inlining hurts maintainability. Only do it manually when profiling **proves** call overhead is the bottleneck.

---

## 8. Strength Reduction `‚öôÔ∏è Compiled (auto)` `üîÑ Manual (interpreted)`

Replace an **expensive operation** with a mathematically equivalent **cheaper one**.

> [!INFO] Compiler note
> Compilers apply strength reduction automatically (e.g., multiply by power-of-2 ‚Üí bit shift). Manual SR is most relevant in interpreted languages or GPU/SIMD code.

| Expensive | Cheaper equivalent | Notes |
|---|---|---|
| `x * 2` | `x << 1` | Bit shift (compiled: auto; Python: negligible gain) |
| `x ** 2` | `x * x` | Avoids `pow()` overhead |
| `x % 2 == 0` | `x & 1 == 0` | Bitwise even check |
| Division by constant | Multiply by reciprocal | Compiler does this; manual in shaders/SIMD |
| `math.pow(x, 2)` | `x * x` or `x**2` | Avoid function call overhead |
| Repeated `dict[key]` lookup | Cache in local variable | Applies to Python/JS |

```python
# Python ‚Äî avoid repeated power/division in loops
# Bad
for i in range(n):
    area = math.pi * radius ** 2   # ** and math.pi lookup every iteration

# Better (combine with loop invariant hoisting)
pi = math.pi
r2 = radius * radius               # strength: ** ‚Üí multiply
for i in range(n):
    area = pi * r2
```

---

## 9. Loop Unrolling `‚öôÔ∏è Compiled (auto/pragma)` `üîÑ Manual (rare)`

Replicate the loop body **multiple times per iteration** to reduce loop control overhead (increment, compare, branch).

> [!INFO] Compiler note
> Compilers unroll loops automatically based on trip count and CPU cache. In C: `#pragma GCC unroll N`. In Rust: LLVM unrolls hot loops. Manual unrolling is rarely needed ‚Äî let the compiler decide.

```c
// Original loop
for (int i = 0; i < n; i++) {
    a[i] = b[i] + c[i];
}

// Unrolled x4 (compiler would do this automatically)
for (int i = 0; i < n - 3; i += 4) {
    a[i]   = b[i]   + c[i];
    a[i+1] = b[i+1] + c[i+1];
    a[i+2] = b[i+2] + c[i+2];
    a[i+3] = b[i+3] + c[i+3];
}
```

```python
# Python ‚Äî manual unrolling rarely helps (interpreter overhead dominates)
# Prefer: use NumPy/vectorized ops instead of unrolling
import numpy as np
result = b_arr + c_arr  # vectorized ‚Äî far better than any manual unrolling
```

> [!WARNING] Python caveat
> Manual loop unrolling in Python almost never helps ‚Äî the interpreter overhead per opcode dwarfs loop control cost. Use NumPy or Cython for hot numeric loops instead.

---

## 10. Register Allocation `‚öôÔ∏è Compiled (auto)` `üåê Awareness`

The compiler assigns frequently used variables to **CPU registers** (fastest storage) instead of memory. This is fully automatic in compiled languages ‚Äî but your coding patterns influence how well the compiler can allocate registers.

> [!INFO] Compiler note
> You cannot manually allocate registers in Python or modern C/C++ (the `register` keyword was removed in C++17). The compiler's register allocator (e.g., LLVM's greedy allocator) decides automatically. Your job: write code that gives the compiler clear, analyzable patterns.

**What helps the compiler allocate registers well:**
- **Local variables** over repeated global/heap access ‚Äî locals are easier to keep in registers
- **Short live ranges** ‚Äî use variables close to where they're defined; don't keep them alive across long blocks
- **Avoid aliasing** ‚Äî if two pointers might refer to the same memory, the compiler can't keep either in a register safely; use `restrict` in C or Rust's borrow checker guarantees this automatically
- **Scalar over struct members** ‚Äî extract frequently used fields into locals

```c
// Bad for register allocation ‚Äî ptr aliasing unclear
void add(int* a, int* b, int* out, int n) {
    for (int i = 0; i < n; i++)
        out[i] = a[i] + b[i];   // compiler unsure if out aliases a or b
}

// Good ‚Äî restrict tells compiler no aliasing
void add(int* restrict a, int* restrict b, int* restrict out, int n) {
    for (int i = 0; i < n; i++)
        out[i] = a[i] + b[i];   // compiler can keep a[i], b[i] in registers
}
```

```python
# Python awareness ‚Äî local variable access is faster than global
import math

# Slower ‚Äî global lookup each call
for x in data:
    y = math.sqrt(x)

# Faster ‚Äî bind to local (register-like effect in CPython's frame)
sqrt = math.sqrt
for x in data:
    y = sqrt(x)
```

---

## 11. Struct Field Reordering (Data Alignment Optimization) `‚öôÔ∏è Compiled (most impactful)` `üåê Awareness`

Order struct/class fields **from largest to smallest** to minimize the **padding bytes** the compiler inserts to satisfy CPU alignment requirements. Smaller structs ‚Üí better cache utilization ‚Üí faster iteration over arrays of structs.

> [!INFO] Why padding exists
> CPUs require data to be **naturally aligned** ‚Äî an 8-byte `double` must start at an address divisible by 8, a 4-byte `int` at a multiple of 4, etc. If fields are in the wrong order, the compiler silently inserts padding bytes between them to satisfy this constraint.

```c
// POOR ALIGNMENT ‚Äî Size: 24 bytes (7 bytes wasted padding)
struct Bad {
    char a;      // 1 byte  [offset 0]
                 // 7 bytes padding ‚Üê compiler inserts this
    double b;    // 8 bytes [offset 8]
    int c;       // 4 bytes [offset 16]
                 // 4 bytes padding at end
};

// GOOD ALIGNMENT ‚Äî Size: 16 bytes (3 bytes wasted padding)
struct Good {
    double b;    // 8 bytes [offset 0]
    int c;       // 4 bytes [offset 8]
    char a;      // 1 byte  [offset 12]
                 // 3 bytes padding at end only
};
```

**Rule of thumb ‚Äî sort fields by size, largest first:**

| Type | Typical size | Alignment |
|---|---|---|
| `double` / `int64_t` / pointer (64-bit) | 8 bytes | 8 |
| `float` / `int32_t` | 4 bytes | 4 |
| `int16_t` / `short` | 2 bytes | 2 |
| `char` / `bool` / `uint8_t` | 1 byte | 1 |

```rust
// Rust ‚Äî use #[repr(C)] for predictable, controlled layout
#[repr(C)]
struct Bad  { a: u8, b: f64, c: i32 }  // 24 bytes
#[repr(C)]
struct Good { b: f64, c: i32, a: u8 }  // 16 bytes

// Without #[repr(C)], Rust's default repr may already reorder automatically
// Use std::mem::size_of::<T>() to verify
```

```python
# Python ‚Äî use __slots__ to eliminate __dict__ overhead
# Order fields largest ‚Üí smallest in numpy structured dtypes

from dataclasses import dataclass

@dataclass
class Good:
    __slots__ = ('b', 'c', 'a')
    b: float   # 8 bytes equivalent
    c: int
    a: int

# For bulk numeric data ‚Äî numpy structured array with explicit dtype
import numpy as np
dtype = np.dtype([('b', np.float64), ('c', np.int32), ('a', np.uint8)])
arr = np.zeros(1_000_000, dtype=dtype)  # tightly packed, cache-friendly
```

> [!TIP] Tools to inspect struct size & padding
> - **C/C++**: `sizeof(Bad)`, `offsetof(Bad, field)`, `pahole` tool
> - **Rust**: `std::mem::size_of::<Good>()`, `std::mem::offset_of!()` (1.77+)
> - **Python**: `sys.getsizeof()`, `arr.dtype.itemsize` for numpy

> [!WARNING] Readability first
> Reorder only when profiling shows struct size is a cache bottleneck (e.g., iterating millions of structs). Group logically related fields together first; split them apart only if measurement justifies it.

---

## Summary

| # | Technique | Applicability | What it avoids |
|---|---|---|---|
| 1 | Short-circuit | üåê Universal | Unnecessary evaluation |
| 2 | Memoization | üåê Universal | Redundant computation |
| 3 | Loop invariant hoisting | ‚öôÔ∏è Auto / üîÑ Manual | Per-iteration redundant work |
| 4 | Branch prediction | ‚öôÔ∏è Compiled + hints / üåê Algorithmic | Pipeline stalls |
| 5 | Common subexpression elim. | ‚öôÔ∏è Auto / üîÑ Manual | Repeated computation |
| 6 | Dead code elimination | ‚öôÔ∏è Auto / üîÑ Manual | Unnecessary work & binary bloat |
| 7 | Function inlining | ‚öôÔ∏è Auto / üîÑ Hot paths only | Call overhead |
| 8 | Strength reduction | ‚öôÔ∏è Auto / üîÑ Manual | Expensive ops (pow, div) |
| 9 | Loop unrolling | ‚öôÔ∏è Auto / üîÑ Rare | Loop control overhead |
| 10 | Register allocation | ‚öôÔ∏è Auto / üåê Awareness | Memory latency |
| 11 | Struct field reordering | ‚öôÔ∏è Compiled / üåê Awareness | Padding waste, cache pressure |

> [!TIP] Priority order for Python
> 1 ‚Üí 2 ‚Üí 3 ‚Üí 5 ‚Üí 8. Use `__slots__` and numpy dtypes for memory layout (11). Techniques 4, 7, 9, 10 are mostly compiler concerns ‚Äî focus on algorithmic improvements and NumPy vectorization instead.

> [!TIP] Priority order for Rust/C++
> The compiler handles 3, 5, 6, 7, 8, 9, 10 automatically. Focus on: 1, 2, 4 (branch hints), 11 (struct layout with `#[repr(C)]`), and cache locality.

## Related Concepts
- [[71_Python_Snippets_MOC]] - Parent category
- [[71.04 Google Python Style Guide]] - Code clarity trumps micro-optimization
