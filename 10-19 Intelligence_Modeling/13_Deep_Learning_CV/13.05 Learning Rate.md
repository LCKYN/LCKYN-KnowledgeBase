---
id: 13.05
tags: [type/concept, status/evergreen, context/dl]
---

# Learning Rate

## Overview

**Learning rate** ($\alpha$ or $\eta$) is a hyperparameter that controls the step size during gradient descent optimization. It determines how much to adjust model weights in response to the estimated error each time the model weights are updated.

## Key Concepts

- **Too high**: Overshoots minimum, may diverge or oscillate
- **Too low**: Slow convergence, may get stuck in local minima
- **Sweet spot**: Fast convergence to good minimum

## Weight Update Formula

$$w_{t+1} = w_t - \alpha \cdot \nabla L(w_t)$$

Where:
- $w_t$ = weights at step $t$
- $\alpha$ = learning rate
- $\nabla L$ = gradient of loss function

## Typical Ranges

| Domain | Common Range |
|--------|--------------|
| SGD | 0.01 - 0.1 |
| Adam | 0.0001 - 0.001 |
| Fine-tuning | 1e-5 - 1e-4 |
| Transfer Learning | 1e-6 - 1e-4 |

## Practical Guidelines

> [!TIP] Finding Good Learning Rate
> - Start with 1e-3 for Adam, 1e-2 for SGD
> - Use learning rate finder (train with exponentially increasing LR, plot loss)
> - Pick LR where loss decreases fastest (before divergence)

> [!WARNING] Common Pitfalls
> - Using same LR for all layers (use differential LR for transfer learning)
> - Not adjusting LR when changing batch size (linear scaling rule)

## Batch Size Relationship

When scaling batch size by $k$, scale learning rate by $\sqrt{k}$ (or linearly for some cases):

| Batch Size | Learning Rate Adjustment |
|------------|-------------------------|
| ×2 | ×1.4 (sqrt) or ×2 (linear) |
| ×4 | ×2 (sqrt) or ×4 (linear) |

## Related Concepts

- [[13_Deep_Learning_CV_MOC]]
- [[13.06 Learning Rate Decay]] - Schedules for LR
- [[13.07 Adam Optimizer]] - Adaptive learning rate
- [[13.09 Gradient Descent]] - LR controls step size
