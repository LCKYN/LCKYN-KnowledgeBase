---
id: 13.09
tags: [type/concept, status/evergreen, context/dl]
---

# Gradient Descent

## Overview

**Gradient Descent** is the fundamental optimization algorithm for minimizing loss functions in machine learning. It iteratively moves parameters in the direction of steepest descent (negative gradient) to find the minimum of a function.

## Core Idea

$$\theta_{t+1} = \theta_t - \alpha \nabla L(\theta_t)$$

Where:
- $\theta$ = model parameters
- $\alpha$ = learning rate
- $\nabla L$ = gradient of loss function

## Variants by Batch Size

| Variant | Data Used | Update Frequency |
|---------|-----------|------------------|
| **Batch GD** | Entire dataset | Once per epoch |
| **Stochastic GD** | Single sample | Every sample |
| **Mini-batch GD** | Subset (32-512) | Every batch |

## Comparison

| Aspect | Batch GD | SGD | Mini-batch |
|--------|----------|-----|------------|
| Stability | High | Low | Medium |
| Speed | Slow | Fast | Fast |
| Memory | High | Low | Medium |
| Parallelization | Good | Poor | Good |
| Escape local minima | Hard | Easy | Medium |

> [!INFO] Modern "SGD"
> When people say "SGD" today, they typically mean mini-batch gradient descent with momentum—not true single-sample updates.

## Convergence Conditions

For convex functions with Lipschitz-continuous gradients:
- **Batch GD**: Converges to global minimum
- **SGD**: Converges to neighborhood of minimum (noise)

| Function Type | Guarantee |
|---------------|-----------|
| Convex | Global minimum |
| Non-convex | Local minimum or saddle point |

## Challenges

| Problem | Description | Solution |
|---------|-------------|----------|
| **Saddle points** | Gradient ≈ 0 but not minimum | Momentum, noise |
| **Ravines** | High curvature in one direction | Momentum, adaptive LR |
| **Plateaus** | Flat regions with tiny gradients | Larger LR, momentum |
| **Local minima** | Stuck in suboptimal solution | SGD noise, restarts |

## Gradient Computation

> [!TIP] Backpropagation
> Gradients are computed efficiently via **backpropagation** using the chain rule. Modern frameworks (PyTorch, TensorFlow) handle this automatically with autograd.

## Learning Rate Impact

```
Loss
  │    ╭─ LR too high (diverges)
  │   ╱
  │  ╱    ╭─── LR too low (slow)
  │ ╱    ╱
  │╱    ╱   ╭── Good LR
  │    ╱   ╱
  └───────────── Iterations
```

## Extensions

| Technique | Purpose |
|-----------|---------|
| **Momentum** | Accelerate in consistent directions |
| **Adaptive LR** | Per-parameter learning rates |
| **LR Scheduling** | Decrease LR over time |
| **Gradient Clipping** | Prevent exploding gradients |

## Gradient Clipping

Prevents exploding gradients by limiting gradient magnitude:

| Method | Description |
|--------|-------------|
| Clip by value | $g = \text{clip}(g, -\tau, \tau)$ |
| Clip by norm | $g = g \cdot \frac{\tau}{\|g\|}$ if $\|g\| > \tau$ |

> [!WARNING] When Gradient Descent Fails
> - **Exploding gradients**: Use gradient clipping, lower LR
> - **Vanishing gradients**: Use skip connections, better initialization
> - **NaN loss**: Check for division by zero, reduce LR

## Related Concepts

- [[13_Deep_Learning_CV_MOC]]
- [[13.08 SGD]] - Stochastic variant
- [[13.07 Adam Optimizer]] - Advanced optimizer
- [[13.05 Learning Rate]] - Step size control
- [[13.01 Vanishing Gradient]] - Gradient flow issue
