---
id: 13.11
tags: [type/concept, status/evergreen, context/dl]
---

# ReLU (Rectified Linear Unit)

**ReLU** is the most widely used activation function in deep learning, defined as the positive part of its argument.

## Definition

$$f(x) = \max(0, x) = \begin{cases} x & \text{if } x > 0 \\ 0 & \text{if } x \leq 0 \end{cases}$$

**Derivative:**
$$f'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x < 0 \end{cases}$$

## Key Properties

| Property | Value |
|----------|-------|
| Range | $[0, \infty)$ |
| Computational cost | Very low (simple threshold) |
| Sparsity | Yes (outputs zero for negative inputs) |
| Gradient | Constant (1) for positive inputs |

## Advantages

- **Computational efficiency**: Simple max operation
- **Sparse activation**: ~50% neurons output zero â†’ efficient representations
- **Mitigates vanishing gradient**: Gradient is 1 for positive values
- **Faster convergence**: Compared to sigmoid/tanh

## Disadvantages

> [!WARNING] Dying ReLU Problem
> Neurons can "die" during training if they always output 0 (negative input region). Once dead, gradients are zero and weights never update.

## Variants

| Variant | Formula | Benefit |
|---------|---------|---------|
| Leaky ReLU | $\max(0.01x, x)$ | Prevents dying neurons |
| PReLU | $\max(\alpha x, x)$ | Learnable slope |
| ELU | $x$ if $x>0$, $\alpha(e^x-1)$ otherwise | Smooth, zero-centered |
| GELU | $x \cdot \Phi(x)$ | Used in Transformers |

## Practical Use Cases

- **Default choice** for hidden layers in CNNs and MLPs
- **Not suitable** for output layers (use sigmoid/softmax instead)

## Related Concepts

- [[13_Deep_Learning_CV_MOC]] - parent category
- [[13.10 Activation Functions]] - overview of activations
- [[13.01 Vanishing Gradient]] - problem ReLU helps mitigate
- [[13.12 Sigmoid]] - contrast with saturating activation
