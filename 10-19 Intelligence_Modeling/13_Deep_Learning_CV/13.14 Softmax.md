---
id: 13.14
tags: [type/concept, status/evergreen, context/dl]
---

# Softmax Function

**Softmax** converts a vector of raw scores (logits) into a probability distribution, ensuring outputs sum to 1.

## Definition

$$\text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$$

Where:
- $z$ = input vector (logits)
- $K$ = number of classes
- Output: probability for each class

## Key Properties

| Property | Value |
|----------|-------|
| Output range | $(0, 1)$ per element |
| Sum of outputs | Exactly 1 |
| Monotonic | Preserves relative ordering |
| Temperature | Controllable sharpness |

## Temperature Scaling

$$\text{softmax}(z_i, T) = \frac{e^{z_i/T}}{\sum_{j} e^{z_j/T}}$$

| Temperature $T$ | Effect |
|-----------------|--------|
| $T < 1$ | Sharper distribution (more confident) |
| $T = 1$ | Standard softmax |
| $T > 1$ | Softer distribution (more uniform) |
| $T \to 0$ | Approaches argmax (one-hot) |
| $T \to \infty$ | Uniform distribution |

> [!TIP] Use Cases for Temperature
> - **Knowledge distillation**: Higher T to soften teacher outputs
> - **Sampling in LLMs**: Control randomness in generation
> - **Calibration**: Adjust confidence of predictions

## Numerical Stability

> [!WARNING] Overflow Prevention
> Subtract max value before computing:
> $$\text{softmax}(z_i) = \frac{e^{z_i - \max(z)}}{\sum_{j} e^{z_j - \max(z)}}$$

## Softmax vs Sigmoid

| Aspect | Softmax | Sigmoid |
|--------|---------|---------|
| Output sum | = 1 (mutually exclusive) | Independent |
| Use case | Multi-class, single label | Binary or multi-label |
| Classes | $K \geq 2$ | Binary |

## Practical Use Cases

- **Multi-class classification**: Final layer of CNNs, Transformers
- **Attention mechanism**: Converting scores to attention weights
- **Reinforcement learning**: Policy networks (action probabilities)

## Jacobian

For output $\hat{y} = \text{softmax}(z)$:
$$\frac{\partial \hat{y}_i}{\partial z_j} = \hat{y}_i(\delta_{ij} - \hat{y}_j)$$

Where $\delta_{ij}$ is Kronecker delta. Combined with cross entropy, gradient simplifies to $\hat{y} - y$.

## Related Concepts

- [[13_Deep_Learning_CV_MOC]] - parent category
- [[13.10 Activation Functions]] - overview of activations
- [[13.12 Sigmoid]] - binary classification alternative
- [[13.13 Cross Entropy]] - loss function paired with softmax
- [[11.01 Attention Mechanism]] - uses softmax for attention weights
