---
id: 13.12
tags: [type/concept, status/evergreen, context/dl]
---

# Sigmoid Function

**Sigmoid** (logistic function) squashes input values to the range $(0, 1)$, historically popular but now mainly used for output layers.

## Definition

$$\sigma(x) = \frac{1}{1 + e^{-x}}$$

**Derivative:**
$$\sigma'(x) = \sigma(x)(1 - \sigma(x))$$

## Key Properties

| Property | Value |
|----------|-------|
| Range | $(0, 1)$ |
| Output at $x=0$ | 0.5 |
| Saturates at | $x < -5$ (≈0) and $x > 5$ (≈1) |
| Max gradient | 0.25 (at $x=0$) |

## Advantages

- **Probabilistic interpretation**: Output represents probability
- **Smooth gradient**: Differentiable everywhere
- **Bounded output**: Useful for gating mechanisms

## Disadvantages

> [!WARNING] Key Problems
> - **Vanishing gradients**: Saturates for large $|x|$, gradient → 0
> - **Not zero-centered**: Outputs always positive, causes zig-zag gradients
> - **Computationally expensive**: Exponential calculation

## Practical Use Cases

| Use Case | Why Sigmoid |
|----------|-------------|
| Binary classification output | Maps logits to probability $P(y=1)$ |
| Gating in LSTMs/GRUs | Controls information flow (0 to 1) |
| Attention weights | Bounded attention scores |

> [!TIP] When to Use
> - **Output layer**: Binary classification
> - **Hidden layers**: Avoid—use ReLU instead
> - **Gating mechanisms**: LSTMs, attention

## Comparison with Tanh

| Aspect | Sigmoid | Tanh |
|--------|---------|------|
| Range | $(0, 1)$ | $(-1, 1)$ |
| Zero-centered | No | Yes |
| Relationship | $\tanh(x) = 2\sigma(2x) - 1$ | - |

## Related Concepts

- [[13_Deep_Learning_CV_MOC]] - parent category
- [[13.10 Activation Functions]] - overview of activations
- [[13.11 ReLU]] - preferred for hidden layers
- [[13.01 Vanishing Gradient]] - main limitation of sigmoid
- [[13.13 Cross Entropy]] - loss function paired with sigmoid
- [[12.04 Logistic Regression]] - uses sigmoid for classification
