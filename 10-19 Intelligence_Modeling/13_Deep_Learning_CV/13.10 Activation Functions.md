---
id: 13.10
tags: [type/concept, status/evergreen, context/dl]
---

# Activation Functions

**Activation functions** introduce non-linearity into neural networks, enabling them to learn complex patterns beyond linear transformations.

## Key Concepts

- **Non-linearity**: Without activation functions, stacking layers would collapse into a single linear transformation
- **Differentiability**: Required for backpropagation (most are differentiable everywhere or almost everywhere)
- **Range**: Output bounds affect gradient flow and numerical stability

## Common Activation Functions

| Function | Range | Use Case | Gradient Issue |
|----------|-------|----------|----------------|
| [[13.11 ReLU]] | $[0, \infty)$ | Hidden layers (default) | Dying ReLU |
| [[13.12 Sigmoid]] | $(0, 1)$ | Binary classification output | Vanishing gradient |
| [[13.14 Softmax]] | $(0, 1)$, sums to 1 | Multi-class output | - |
| Tanh | $(-1, 1)$ | Hidden layers, RNNs | Vanishing gradient |
| Leaky ReLU | $(-\infty, \infty)$ | Hidden layers | - |

## Choosing Activation Functions

> [!TIP] Best Practices
> - **Hidden layers**: Start with ReLU or its variants (Leaky ReLU, ELU)
> - **Binary output**: Sigmoid
> - **Multi-class output**: Softmax
> - **Regression output**: Linear (no activation)

## Mathematical Properties

For activation function $f(x)$:
- **Monotonic**: Helps gradient-based optimization
- **Zero-centered**: Tanh preferred over Sigmoid for hidden layers
- **Bounded vs Unbounded**: Trade-off between saturation and exploding activations

## Related Concepts

- [[13_Deep_Learning_CV_MOC]] - parent category
- [[13.01 Vanishing Gradient]] - problem with saturating activations
- [[13.11 ReLU]] - most common hidden layer activation
- [[13.12 Sigmoid]] - classic activation, used in output layer
- [[13.14 Softmax]] - multi-class probability output
