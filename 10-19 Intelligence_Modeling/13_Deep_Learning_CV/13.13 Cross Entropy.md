---
id: 13.13
tags: [type/concept, status/evergreen, context/dl]
---

# Cross Entropy Loss

**Cross entropy** measures the difference between two probability distributionsâ€”used as the standard loss function for classification tasks.

## Definition

**Binary Cross Entropy** (for 2 classes):
$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \left[ y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i) \right]$$

**Categorical Cross Entropy** (for $C$ classes):
$$\mathcal{L} = -\frac{1}{N}\sum_{i=1}^{N} \sum_{c=1}^{C} y_{i,c} \log(\hat{y}_{i,c})$$

Where:
- $y$ = ground truth (one-hot encoded)
- $\hat{y}$ = predicted probability
- $N$ = number of samples

## Key Concepts

| Concept | Description |
|---------|-------------|
| Information theory | Measures "surprise" of predictions |
| Lower bound | 0 (perfect predictions) |
| Numerical stability | Add small $\epsilon$ to avoid $\log(0)$ |

## Why Cross Entropy for Classification?

> [!INFO] Intuition
> - Penalizes **confident wrong predictions** heavily ($\log(0.01) \approx -4.6$)
> - Rewards **confident correct predictions** ($\log(0.99) \approx -0.01$)
> - Provides stronger gradients than MSE for classification

## Comparison with MSE

| Aspect | Cross Entropy | MSE |
|--------|---------------|-----|
| Gradient magnitude | Large when wrong | Small when saturated |
| Probabilistic | Yes (log-likelihood) | No |
| Use case | Classification | Regression |

## Practical Pairing

| Task | Output Activation | Loss Function |
|------|-------------------|---------------|
| Binary classification | Sigmoid | Binary Cross Entropy |
| Multi-class (single label) | Softmax | Categorical Cross Entropy |
| Multi-label | Sigmoid (per class) | Binary Cross Entropy (sum) |

> [!TIP] Implementation
> Use combined `CrossEntropyLoss` (PyTorch) or `softmax_cross_entropy` which applies softmax + cross entropy together for numerical stability.

## Gradient

For softmax output with cross entropy:
$$\frac{\partial \mathcal{L}}{\partial z_i} = \hat{y}_i - y_i$$

This elegant gradient makes training efficient and stable.

## Related Concepts

- [[13_Deep_Learning_CV_MOC]] - parent category
- [[13.14 Softmax]] - paired activation for multi-class
- [[13.12 Sigmoid]] - paired activation for binary
- [[13.09 Gradient Descent]] - optimization using this loss
