---
id: 13.01
tags: [type/concept, status/seed, context/dl]
---

# Vanishing Gradient Problem

## Definition
In deep neural networks, the **Vanishing Gradient Problem** occurs during backpropagation when gradients used to update the network weights become extremely small (approaching zero). This effectively stops the network from learning, as the weights in earlier layers are hardly updated.

## Causes
- **Activation Functions**: Functions like Sigmoid or Tanh squash inputs into a small range (0 to 1 or -1 to 1). Their derivatives are always less than 1.
- **Deep Networks**: Repeated multiplication of small derivatives (chain rule) across many layers causes the gradient to decay exponentially.

## Solutions
- **ReLU (Rectified Linear Unit)**: Derivative is either 0 or 1, preventing decay for positive inputs.
- **Weight Initialization**: He or Xavier initialization.
- **Residual Connections (ResNets)**: Skip connections allow gradients to flow directly through the network.
- **Batch Normalization**: Normalizes layer inputs to maintain stable distributions.
- **Architectures**: LSTM/GRU for sequence data (gates control gradient flow).

## Impact on Deep Architectures
The vanishing gradient problem significantly influenced the design of deep CNN architectures:
- **[[13.02 VGG]]**: Used ReLU and careful initialization to train 16-19 layer networks, but still faced gradient flow challenges without skip connections
- **ResNet**: Introduced residual connections to enable training of 50-152 layer networks
- **Highway Networks**: Learned gating mechanisms for gradient flow

## Related Concepts

- [[13_Deep_Learning_CV_MOC]]
- [[13.03 Batch Normalization]] - Helps stabilize gradients
- [[13.09 Gradient Descent]] - Optimization affected by vanishing
