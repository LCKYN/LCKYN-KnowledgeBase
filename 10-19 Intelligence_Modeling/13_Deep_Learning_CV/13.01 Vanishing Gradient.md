---
id: 13.01
tags: [type/concept, status/seed, context/dl]
---

# Vanishing Gradient Problem

## Definition
In deep neural networks, the **Vanishing Gradient Problem** occurs during backpropagation when gradients used to update the network weights become extremely small (approaching zero). This effectively stops the network from learning, as the weights in earlier layers are hardly updated.

## Causes
- **Activation Functions**: Functions like Sigmoid or Tanh squash inputs into a small range (0 to 1 or -1 to 1). Their derivatives are always less than 1.
- **Deep Networks**: Repeated multiplication of small derivatives (chain rule) across many layers causes the gradient to decay exponentially.

## Solutions
- **ReLU (Rectified Linear Unit)**: Derivative is either 0 or 1, preventing decay for positive inputs.
- **Weight Initialization**: He or Xavier initialization.
- **Residual Connections (ResNets)**: Skip connections allow gradients to flow directly through the network.
- **Batch Normalization**: Normalizes layer inputs to maintain stable distributions.
- **Architectures**: LSTM/GRU for sequence data (gates control gradient flow).
