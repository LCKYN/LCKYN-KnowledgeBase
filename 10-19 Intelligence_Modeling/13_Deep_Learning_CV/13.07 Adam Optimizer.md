---
id: 13.07
tags: [type/concept, status/evergreen, context/dl]
---

# Adam Optimizer

## Overview

**Adam** (Adaptive Moment Estimation) is an optimization algorithm that combines the benefits of AdaGrad and RMSprop. It maintains per-parameter adaptive learning rates using estimates of first and second moments of gradients.

## Key Features

- **Adaptive LR**: Different learning rate for each parameter
- **Momentum**: Uses exponential moving average of gradients (1st moment)
- **RMSprop-like**: Uses exponential moving average of squared gradients (2nd moment)
- **Bias correction**: Corrects initialization bias in moment estimates

## Algorithm

1. Compute gradient: $g_t = \nabla L(\theta_t)$
2. Update 1st moment: $m_t = \beta_1 m_{t-1} + (1-\beta_1)g_t$
3. Update 2nd moment: $v_t = \beta_2 v_{t-1} + (1-\beta_2)g_t^2$
4. Bias correction: $\hat{m}_t = \frac{m_t}{1-\beta_1^t}$, $\hat{v}_t = \frac{v_t}{1-\beta_2^t}$
5. Update params: $\theta_{t+1} = \theta_t - \frac{\alpha}{\sqrt{\hat{v}_t} + \epsilon}\hat{m}_t$

## Default Hyperparameters

| Parameter | Default | Description |
|-----------|---------|-------------|
| $\alpha$ (lr) | 0.001 | Learning rate |
| $\beta_1$ | 0.9 | 1st moment decay |
| $\beta_2$ | 0.999 | 2nd moment decay |
| $\epsilon$ | 1e-8 | Numerical stability |

## Adam Variants

| Variant | Key Difference |
|---------|---------------|
| **AdamW** | Decoupled weight decay (preferred for transformers) |
| **NAdam** | Nesterov momentum incorporated |
| **RAdam** | Rectified Adam, handles variance early training |
| **AdaFactor** | Memory efficient, factorized 2nd moments |

> [!WARNING] Adam vs AdamW
> Standard Adam applies L2 regularization incorrectly. **AdamW** properly decouples weight decay from gradient updates. Always prefer AdamW for modern architectures.

## When to Use

| Optimizer | Best For |
|-----------|----------|
| **Adam/AdamW** | Default choice, transformers, most DL |
| **SGD+Momentum** | CNNs, when compute budget allows tuning |
| **AdaFactor** | Large models (memory constrained) |

> [!TIP] Practical Advice
> - Start with AdamW, lr=1e-4 to 3e-4
> - Use weight_decay=0.01 for regularization
> - Switch to SGD if Adam overfits or for final fine-tuning

## Comparison

| Aspect | SGD+Momentum | Adam |
|--------|--------------|------|
| Tuning effort | High | Low |
| Convergence speed | Slower | Faster |
| Generalization | Often better | Good |
| Memory | Lower | 2Ã— more |

## Related Concepts

- [[13.05 Learning Rate]]
- [[13.06 Learning Rate Decay]]
- [[13.01 Vanishing Gradient]]
- [[12.09 Hyperparameter Tuning]]
