---
id: 13.06
tags: [type/concept, status/evergreen, context/dl]
---

# Learning Rate Decay

## Overview

**Learning rate decay** (or learning rate scheduling) is a technique that reduces the learning rate during training. Starting with a larger LR enables fast initial progress, while decreasing it allows fine-grained convergence near the optimum.

## Common Schedules

| Schedule | Formula | Use Case |
|----------|---------|----------|
| Step Decay | $\alpha_t = \alpha_0 \cdot \gamma^{\lfloor t/s \rfloor}$ | Simple, predictable drops |
| Exponential | $\alpha_t = \alpha_0 \cdot e^{-kt}$ | Smooth continuous decay |
| Cosine Annealing | $\alpha_t = \alpha_{min} + \frac{1}{2}(\alpha_0 - \alpha_{min})(1 + \cos(\frac{t\pi}{T}))$ | State-of-the-art results |
| Linear | $\alpha_t = \alpha_0 \cdot (1 - \frac{t}{T})$ | Transformers, fine-tuning |
| ReduceOnPlateau | Reduce when metric stops improving | Adaptive, metric-based |

## Warmup

> [!INFO] Learning Rate Warmup
> Start with small LR and gradually increase to target LR over first few epochs. Helps stabilize early training, especially with large batch sizes.

**Linear warmup formula:**
$$\alpha_t = \alpha_{target} \cdot \frac{t}{t_{warmup}}$$

## Popular Combinations

| Strategy | Description |
|----------|-------------|
| Warmup + Cosine | Warmup for 5-10% of training, then cosine decay |
| Warmup + Linear | Common in Transformers (BERT, GPT) |
| Cyclic LR | Oscillate between bounds, can escape local minima |
| One Cycle | Single cycle: warmup → max → decay to near-zero |

## PyTorch Schedulers

```python
# Step decay every 30 epochs
scheduler = StepLR(optimizer, step_size=30, gamma=0.1)

# Cosine annealing
scheduler = CosineAnnealingLR(optimizer, T_max=100)

# Warmup + cosine (manual or use transformers library)
scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)
```

> [!TIP] Best Practices
> - Use warmup for large batch sizes or when training from scratch
> - Cosine annealing often outperforms step decay
> - For fine-tuning: linear decay with warmup works well

## Related Concepts

- [[13_Deep_Learning_CV_MOC]]
- [[13.05 Learning Rate]] - Base concept
- [[13.07 Adam Optimizer]] - Has built-in adaptation
- [[12.09 Hyperparameter Tuning]] - Decay schedule selection
