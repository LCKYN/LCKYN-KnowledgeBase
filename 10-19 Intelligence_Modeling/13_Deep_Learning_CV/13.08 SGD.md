---
id: 13.08
tags: [type/concept, status/evergreen, context/dl]
---

# SGD (Stochastic Gradient Descent)

## Overview

**Stochastic Gradient Descent** is the foundational optimization algorithm for training neural networks. Unlike batch gradient descent that uses the entire dataset, SGD updates weights using a single sample or mini-batch, introducing beneficial noise that helps escape local minima.

## Variants

| Variant | Update Rule | Description |
|---------|-------------|-------------|
| **Vanilla SGD** | $\theta_{t+1} = \theta_t - \alpha \nabla L$ | Basic gradient step |
| **SGD + Momentum** | $v_t = \gamma v_{t-1} + \alpha \nabla L$ | Accumulates velocity |
| **Nesterov (NAG)** | Look-ahead gradient | Momentum with correction |

## Momentum

Momentum accelerates convergence by accumulating a velocity vector in directions of persistent gradients:

$$v_t = \gamma v_{t-1} + \alpha \nabla L(\theta_t)$$
$$\theta_{t+1} = \theta_t - v_t$$

| Parameter | Typical Value |
|-----------|---------------|
| Learning rate $\alpha$ | 0.01 - 0.1 |
| Momentum $\gamma$ | 0.9 - 0.99 |

> [!INFO] Why Momentum Works
> - Dampens oscillations in ravines (high curvature directions)
> - Accelerates movement in consistent gradient directions
> - Acts like a ball rolling downhill with inertia

## Nesterov Accelerated Gradient

Computes gradient at the "look-ahead" position:

$$v_t = \gamma v_{t-1} + \alpha \nabla L(\theta_t - \gamma v_{t-1})$$

> [!TIP] Nesterov vs Standard Momentum
> Nesterov often converges faster because it "looks ahead" before making the update, providing a correction factor.

## Mini-Batch Size

| Batch Size | Trade-off |
|------------|-----------|
| Small (16-32) | More noise, better generalization, slower |
| Medium (64-256) | Good balance |
| Large (512+) | Faster training, may need LR scaling |

## SGD vs Adam

| Aspect | SGD+Momentum | Adam |
|--------|--------------|------|
| Hyperparameter sensitivity | High | Low |
| Generalization | Often better | Good |
| Convergence speed | Slower | Faster |
| Memory overhead | Low | 2Ã— parameters |
| Best for | CNNs, final fine-tuning | Transformers, quick prototyping |

> [!WARNING] Common Mistakes
> - Using too high LR without warmup
> - Not using momentum (vanilla SGD is rarely optimal)
> - Fixed LR throughout training (use scheduling)

## When to Choose SGD

- **Image classification** (ResNet, EfficientNet) - often outperforms Adam
- **When generalization matters** more than training speed
- **Final fine-tuning** after initial Adam training
- **Limited memory** scenarios

## PyTorch Example

```python
optimizer = torch.optim.SGD(
    model.parameters(),
    lr=0.1,
    momentum=0.9,
    weight_decay=1e-4,
    nesterov=True
)
```

## Related Concepts

- [[13_Deep_Learning_CV_MOC]]
- [[13.07 Adam Optimizer]] - Adaptive alternative
- [[13.05 Learning Rate]] - Critical for SGD tuning
- [[13.09 Gradient Descent]] - SGD is stochastic variant
