---
id: 11.18
tags: [type/concept, status/evergreen, context/llm]
---

# LLM Throughput & Memory Bound

## Overview
LLM inference performance is governed by two fundamental bottlenecks: **compute-bound** (not enough FLOPS) and **memory-bound** (not enough memory bandwidth). Understanding which bottleneck dominates at each stage of inference is key to optimizing throughput, latency, and cost.

## Two Phases of LLM Inference

| Phase | Also Called | Bottleneck | Why |
|---|---|---|---|
| **Prefill** | Prompt processing | **Compute-bound** | Processes all input tokens in parallel — high FLOPS demand |
| **Decode** | Token generation | **Memory-bound** | Generates one token at a time — loads entire model weights per token |

```
[Prompt tokens] ──► Prefill (compute-bound) ──► [First token]
                                                      │
                                    Decode loop (memory-bound)
                                      │    │    │    │
                                     t1   t2   t3  ...
```

## Compute-Bound vs Memory-Bound

| Aspect | Compute-Bound | Memory-Bound |
|---|---|---|
| Limiting factor | GPU FLOPS (compute units) | Memory bandwidth (GB/s) |
| Symptom | GPU compute at 100%, memory idle | Memory bandwidth saturated, compute underutilized |
| LLM phase | Prefill, large batch decode | Single-request decode |
| Fix | Faster GPU, quantization, FlashAttention | Higher bandwidth (HBM3), batching, KV cache optimization |

## Arithmetic Intensity

**Arithmetic intensity** = FLOPS / Bytes transferred

$$\text{AI} = \frac{\text{Operations (FLOPS)}}{\text{Memory Access (Bytes)}}$$

- **High AI** → Compute-bound (matrix multiplication in prefill)
- **Low AI** → Memory-bound (loading weights for single token decode)

> [!INFO] The Roofline Model
> A GPU has a **compute ceiling** (peak TFLOPS) and a **memory ceiling** (bandwidth × arithmetic intensity). The actual throughput is the minimum of the two. Decode is almost always under the memory roof.

## Key Metrics

| Metric | Definition | Unit |
|---|---|---|
| **TTFT** | Time to First Token (prefill latency) | ms |
| **TPOT** | Time Per Output Token (decode latency) | ms/token |
| **Throughput** | Total tokens generated per second | tokens/s |
| **TPS per user** | Tokens per second experienced per user | tokens/s |
| **Batch throughput** | Aggregate tokens/s across all concurrent requests | tokens/s |

## Why Decode is Memory-Bound

For each generated token, the GPU must:
1. **Load all model weights** from HBM → compute units (~2 bytes/param for FP16)
2. **Load KV cache** for all previous tokens
3. **Perform a small matmul** (one token × weight matrix)

For a 70B model in FP16:
- Weights: $70B \times 2$ bytes = **140 GB** loaded per token
- GPU HBM bandwidth: ~2 TB/s (A100) → **~14 tokens/s** theoretical max (single request)

> [!WARNING] Memory bandwidth is the ceiling
> No amount of compute optimization helps if you're waiting on weight transfers. This is why **batching** is critical — amortize weight loads across multiple requests.

## Optimization Strategies

### Increase Throughput (Memory-Bound Phase)

| Strategy | How It Helps |
|---|---|
| **Batching** | Load weights once, compute for N requests — higher arithmetic intensity |
| **Continuous batching** | Add new requests to running batch without waiting for all to finish |
| **Quantization** (INT8/INT4) | Smaller weights → fewer bytes to transfer → more tokens/s |
| **KV Cache compression** | Reduce cache size → less memory bandwidth per token |
| **Speculative decoding** | Draft model proposes N tokens, large model verifies in one pass |
| **PagedAttention** (vLLM) | Efficient KV cache memory management, reduces fragmentation |

### Reduce TTFT (Compute-Bound Phase)

| Strategy | How It Helps |
|---|---|
| **FlashAttention** | Fused kernel, reduces memory reads during attention |
| **Tensor parallelism** | Split model across GPUs, parallel matmuls |
| **Prompt caching** | Reuse KV cache for common prompt prefixes |
| **Chunked prefill** | Interleave prefill chunks with decode to avoid stalling |

## Batching Impact

| Batch Size | Phase Bottleneck | Throughput | Latency per Request |
|---|---|---|---|
| 1 | Memory-bound | Low | Lowest |
| 8–16 | Transitioning | Medium | Slightly higher |
| 32–128 | Compute-bound | Highest | Higher |
| Too large | OOM / KV cache limit | Drops | Spikes |

> [!TIP] The batching sweet spot
> Increase batch size until either: (1) GPU compute saturates, (2) KV cache fills available memory, or (3) latency SLA is breached. Tools like **vLLM** and **TGI** handle this automatically with continuous batching.

## Quantization vs Throughput

| Precision | Bytes/Param | 70B Model Size | Decode Speedup |
|---|---|---|---|
| FP16 | 2 | 140 GB | 1× (baseline) |
| INT8 | 1 | 70 GB | ~2× |
| INT4 (GPTQ/AWQ) | 0.5 | 35 GB | ~3–4× |

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.01 Attention Mechanism]] - Compute cost of attention scales with sequence length
- [[11.16 Context Window Management]] - KV cache size grows with context length
- [[11.17 Mixture of Experts]] - MoE reduces active compute per token
- [[11.15 LLM Evaluation Metrics]] - Throughput as a serving metric
