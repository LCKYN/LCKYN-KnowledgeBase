---
id: 11.18
tags: [type/concept, status/evergreen, context/llm]
---

# LLM Throughput & Memory Bound

## Overview
LLM inference performance is governed by two fundamental bottlenecks: **compute-bound** (not enough FLOPS) and **memory-bound** (not enough memory bandwidth). Understanding which bottleneck dominates at each stage of inference is key to optimizing throughput, latency, and cost.

## Two Phases of LLM Inference

| Phase | Also Called | Bottleneck | Why |
|---|---|---|---|
| **Prefill** | Prompt processing | **Compute-bound** | Processes all input tokens in parallel — high FLOPS demand |
| **Decode** | Token generation | **Memory-bound** | Generates one token at a time — loads entire model weights per token |

```
[Prompt tokens] ──► Prefill (compute-bound) ──► [First token]
                                                      │
                                    Decode loop (memory-bound)
                                      │    │    │    │
                                     t1   t2   t3  ...
```

## Compute-Bound vs Memory-Bound

| Aspect | Compute-Bound | Memory-Bound |
|---|---|---|
| Limiting factor | GPU FLOPS (compute units) | Memory bandwidth (GB/s) |
| Symptom | GPU compute at 100%, memory idle | Memory bandwidth saturated, compute underutilized |
| LLM phase | Prefill, large batch decode | Single-request decode |
| Fix | Faster GPU, quantization, FlashAttention | Higher bandwidth (HBM3), batching, KV cache optimization |

## Arithmetic Intensity

**Arithmetic intensity** = FLOPS / Bytes transferred

$$\text{AI} = \frac{\text{Operations (FLOPS)}}{\text{Memory Access (Bytes)}}$$

- **High AI** → Compute-bound (matrix multiplication in prefill)
- **Low AI** → Memory-bound (loading weights for single token decode)

> [!INFO] The Roofline Model
> A GPU has a **compute ceiling** (peak TFLOPS) and a **memory ceiling** (bandwidth × arithmetic intensity). The actual throughput is the minimum of the two. Decode is almost always under the memory roof.

## Key Metrics

| Metric | Definition | Unit |
|---|---|---|
| **TTFT** | Time to First Token (prefill latency) | ms |
| **TPOT** | Time Per Output Token (decode latency) | ms/token |
| **Throughput** | Total tokens generated per second | tokens/s |
| **TPS per user** | Tokens per second experienced per user | tokens/s |
| **Batch throughput** | Aggregate tokens/s across all concurrent requests | tokens/s |

## Why Decode is Memory-Bound

For each generated token, the GPU must:
1. **Load all model weights** from HBM → compute units (~2 bytes/param for FP16)
2. **Load KV cache** for all previous tokens
3. **Perform a small matmul** (one token × weight matrix)

For a 70B model in FP16:
- Weights: $70B \times 2$ bytes = **140 GB** loaded per token
- GPU HBM bandwidth: ~2 TB/s (A100) → **~14 tokens/s** theoretical max (single request)

> [!WARNING] Memory bandwidth is the ceiling
> No amount of compute optimization helps if you're waiting on weight transfers. This is why **batching** is critical — amortize weight loads across multiple requests.

## Optimization Strategies

### Increase Throughput (Memory-Bound Phase)

| Strategy | How It Helps |
|---|---|
| **Batching** | Load weights once, compute for N requests — higher arithmetic intensity |
| **Continuous batching** | Add new requests to running batch without waiting for all to finish |
| **Quantization** (INT8/INT4) | Smaller weights → fewer bytes to transfer → more tokens/s |
| **KV Cache compression** | Reduce cache size → less memory bandwidth per token |
| **Speculative decoding** | Draft model proposes N tokens, large model verifies in one pass |
| **PagedAttention** (vLLM) | Efficient KV cache memory management, reduces fragmentation |

### Reduce TTFT (Compute-Bound Phase)

| Strategy | How It Helps |
|---|---|
| **FlashAttention** | Fused kernel, reduces memory reads during attention |
| **Tensor parallelism** | Split model across GPUs, parallel matmuls |
| **Prompt caching** | Reuse KV cache for common prompt prefixes |
| **Chunked prefill** | Interleave prefill chunks with decode to avoid stalling |

## Batching Impact

| Batch Size | Phase Bottleneck | Throughput | Latency per Request |
|---|---|---|---|
| 1 | Memory-bound | Low | Lowest |
| 8–16 | Transitioning | Medium | Slightly higher |
| 32–128 | Compute-bound | Highest | Higher |
| Too large | OOM / KV cache limit | Drops | Spikes |

> [!TIP] The batching sweet spot
> Increase batch size until either: (1) GPU compute saturates, (2) KV cache fills available memory, or (3) latency SLA is breached. Tools like **vLLM** and **TGI** handle this automatically with continuous batching.

## Quantization vs Throughput

### Weight Quantization

| Precision | Bytes/Param | 70B Model Size | Decode Speedup |
|---|---|---|---|
| FP16 | 2 | 140 GB | 1× (baseline) |
| INT8 | 1 | 70 GB | ~2× |
| INT4 (GPTQ/AWQ) | 0.5 | 35 GB | ~3–4× |

### KV Cache Quantization

The **KV cache** stores key and value tensors for every token in every layer. For long contexts and large batches, it dominates memory usage — often more than the model weights themselves.

$$\text{KV Cache Size} = 2 \times n_{\text{layers}} \times n_{\text{heads}} \times d_{\text{head}} \times \text{seq\_len} \times \text{batch\_size} \times \text{bytes\_per\_element}$$

For **Llama 3 70B** (80 layers, FP16, 8K context, batch=32):
$$2 \times 80 \times 8192 \times 32 \times 2\text{ B} \approx 84\text{ GB}$$

Quantizing the KV cache trades a small quality loss for large memory savings:

| KV Precision | Bytes/Element | KV Cache vs FP16 | Quality Impact |
|---|---|---|---|
| FP16 | 2 | 1× (baseline) | None |
| FP8 (E4M3 / E5M2) | 1 | **50% reduction** | Minimal (~0.1–0.3 perplexity) |
| INT8 | 1 | **50% reduction** | Minimal |
| INT4 | 0.5 | **75% reduction** | Moderate — task-dependent |

> [!INFO] Keys vs Values behave differently
> Research (e.g., **KIVI**, **KVQuant**) shows that **keys** are more sensitive to quantization than **values** because of outlier channels. Common strategy: quantize values aggressively (INT4), keep keys at INT8 or FP8.

#### Techniques

| Technique | Idea | Precision | Notes |
|---|---|---|---|
| **Naive per-tensor** | Single scale factor for entire tensor | INT8 | Fast but poor quality at INT4 |
| **Per-token quantization** | Scale per token vector | INT8/FP8 | Better than per-tensor |
| **Per-channel / grouped** | Scale per head or channel group | INT4/INT8 | Best quality-compression tradeoff |
| **KIVI** | Key INT2 with residual, Value INT2 | INT2 | Research; quality loss noticeable |
| **KVQuant** | Non-uniform quantization, outlier handling | INT4 | Near-lossless at INT4 |
| **MLA (Multi-head Latent Attention)** | DeepSeek — compress KV into low-rank latent vector before caching | ~1/8× KV size | Architecture change, not post-hoc quant |

#### Framework Support

| Framework | KV Cache Quantization | Precision |
|---|---|---|
| **vLLM** | FP8 KV cache (Hopper/Ada GPUs) | FP8 E4M3 |
| **TGI (HuggingFace)** | FP8 / INT8 | FP8 |
| **llama.cpp** | `--cache-type-k`, `--cache-type-v` flags | q4_0, q8_0, f16, f32 |
| **TensorRT-LLM** | FP8 KV cache | FP8 |
| **SGLang** | FP8 KV cache | FP8 |

```bash
# llama.cpp — quantize KV cache independently
./llama-server \
  --model llama-3-70b.gguf \
  --cache-type-k q8_0 \   # keys at INT8
  --cache-type-v q4_0     # values at INT4
```

```python
# vLLM — FP8 KV cache (requires Hopper/Ada GPU: H100, L40S, RTX 4090)
from vllm import LLM

llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    kv_cache_dtype="fp8",       # or "fp8_e4m3", "auto"
    dtype="bfloat16",           # weight dtype unchanged
)
```

> [!TIP] Combined strategy
> Stack KV cache quantization with weight quantization for maximum efficiency:
> - Weights: INT4/AWQ → reduces weight-transfer bottleneck
> - KV cache: FP8 → doubles effective context capacity or batch size
> - Result: serve 2–4× more concurrent users on the same hardware

> [!WARNING] GPU requirements
> FP8 hardware acceleration requires **Hopper (H100)** or **Ada (RTX 4090/L40S)** GPUs. On older GPUs (A100, V100), FP8 is emulated and may be slower than FP16.

## Speculative Decoding

Decode is memory-bound because the target model generates **one token at a time**, wasting FLOPS capacity. **Speculative decoding** fixes this by using a cheap **draft model** to guess several tokens ahead, then running the expensive **target model** once to verify all candidates in parallel — turning sequential decoding into a batch verification step.

### Core Mechanism

```
Draft model (cheap, small):
  → generates γ candidate tokens [d1, d2, d3, d4] autoregressively

Target model (large):
  → runs one forward pass over [prompt + d1 + d2 + d3 + d4] in parallel
  → for each position: accept dᵢ if p_target(dᵢ) ≥ threshold, else resample and stop

Outcome: if all 4 accepted → 4 tokens per target model call
         if 2 accepted     → 2 tokens per target model call (still faster than 1)
```

> [!INFO] Why it's lossless by default
> Using **rejection sampling** (not greedy accept), the output distribution is **identical** to running the target model alone — there's no quality loss, only speed gain.

### Speedup Math

Let $\gamma$ = draft tokens per step, $\alpha$ = acceptance rate (0–1):

$$\text{Expected tokens per target call} = \frac{1 - \alpha^{\gamma+1}}{1 - \alpha}$$

$$\text{Speedup} \approx \frac{\gamma \cdot \alpha}{1} \quad \text{(simplified, when } \alpha \text{ is high)}$$

| Draft tokens (γ) | Acceptance rate (α) | Expected gain | Real-world speedup |
|---|---|---|---|
| 4 | 0.9 | ~3.6× | **2–3×** |
| 4 | 0.7 | ~2.4× | **1.5–2×** |
| 8 | 0.9 | ~6.5× | **3–4×** (on fast hardware) |

> [!WARNING] Speedup is only latency, not throughput
> Speculative decoding improves **single-request latency** (TPOT). For batched serving, it may *reduce* throughput since the draft model consumes extra GPU memory and compute that could serve more requests.

### Variants

| Variant | Draft Source | Notes |
|---|---|---|
| **Classic (Leviathan et al. 2023)** | Separate small model (e.g., 7B drafts for 70B) | Needs aligned vocab/tokenizer |
| **Self-speculative / EAGLE** | Target model's own early layers | No separate model; EAGLE uses feature extrapolation |
| **MEDUSA** | Multiple parallel heads on target model | Adds N decoding heads, no separate model |
| **Lookahead Decoding** | Jacobi iteration — parallel n-gram draft | No draft model at all; ~1.5–2× |
| **Prompt Lookup Decoding** | Copy candidate tokens from the input prompt | Ultra-cheap; great for summarization, RAG, code editing |
| **Batch Speculative Decoding** | Draft shared across batch | Works with continuous batching |

### When to Use

| Scenario | Use Speculative? | Reason |
|---|---|---|
| Low-latency single-user chat | ✅ Yes | High α on conversational text; directly reduces TPOT |
| Summarization / RAG (long input) | ✅ Prompt Lookup | Candidates copied from prompt; minimal overhead |
| High-throughput batch serving | ⚠️ Careful | Draft model adds overhead; benchmark first |
| Code generation | ✅ Yes | Code is repetitive; acceptance rates tend to be high |
| Creative/diverse generation | ❌ Marginal | Low α on diverse outputs; overhead may not pay off |

### Framework Support

| Framework | Support | Config |
|---|---|---|
| **vLLM** | ✅ | `speculative_model`, `num_speculative_tokens` |
| **TGI** | ✅ | `--speculate` flag |
| **TensorRT-LLM** | ✅ | `medusa_choices`, draft model API |
| **llama.cpp** | ✅ | `--draft-model` flag |
| **Hugging Face `generate()`** | ✅ | `assistant_model` parameter |

```python
# HuggingFace — classic speculative decoding
from transformers import AutoModelForCausalLM, AutoTokenizer

draft = AutoModelForCausalLM.from_pretrained("Llama-3.2-1B")
target = AutoModelForCausalLM.from_pretrained("Llama-3.1-70B")
tokenizer = AutoTokenizer.from_pretrained("Llama-3.1-70B")

inputs = tokenizer("The quick brown fox", return_tensors="pt")
outputs = target.generate(
    **inputs,
    assistant_model=draft,    # speculative decoding
    do_sample=False,
    max_new_tokens=200,
)
```

```python
# vLLM — speculative decoding with draft model
from vllm import LLM, SamplingParams

llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    speculative_model="meta-llama/Llama-3.2-1B-Instruct",
    num_speculative_tokens=5,
)
```

```python
# vLLM — Prompt Lookup Decoding (no draft model, copies from prompt)
llm = LLM(
    model="meta-llama/Llama-3.1-70B-Instruct",
    speculative_model="[prompt_lookup_decoding]",
    num_speculative_tokens=5,
    max_model_len=4096,
)
```

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.01 Attention Mechanism]] - Compute cost of attention scales with sequence length
- [[11.16 Context Window Management]] - KV cache size grows with context length
- [[11.21 Prompt Caching]] - Reuse KV cache across requests (orthogonal to quantization)
- [[11.17 Mixture of Experts]] - MoE reduces active compute per token
- [[11.15 LLM Evaluation Metrics]] - Throughput as a serving metric
