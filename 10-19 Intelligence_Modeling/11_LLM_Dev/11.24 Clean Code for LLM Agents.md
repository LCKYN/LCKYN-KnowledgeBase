---
id: 11.24
tags: [type/concept, status/evergreen, context/llm]
---

# Clean Code for LLM Agents

## Overview
LLM agent code has unique failure modes that standard clean code practices don't fully address: **non-deterministic outputs**, **opaque tool calls**, **prompt-logic coupling**, and **hidden state in memory**. This note captures patterns specific to building maintainable, debuggable agent systems.

---

## Core Principles

| Principle | Agent-Specific Meaning |
|---|---|
| **Explicit over implicit** | System prompt logic, tool routing, and memory access should be visible, not buried in framework magic |
| **Fail loud** | Agent failures (tool errors, parsing failures, loop termination) should surface immediately, not silently produce wrong answers |
| **Reproducibility first** | Log inputs, outputs, tool calls, and intermediate reasoning so any run can be replayed |
| **Separate concerns** | Prompt = intent; Tool = capability; Orchestrator = control flow; Memory = persistence |
| **Testability** | Each component (prompt template, tool function, parser) is individually testable without the full agent |

---

## 1. Separate Prompt from Code

**Anti-pattern**: embedding prompt strings inside orchestration logic.

```python
# ❌ Prompt logic tangled with control flow
def run_agent(user_input: str):
    response = llm.call(
        f"You are a helpful assistant. Today is {datetime.now():%Y-%m-%d}. "
        f"The user said: {user_input}. "
        "Use the tools available to respond helpfully."
    )
```

```python
# ✅ Prompts are structured, versioned, and loaded separately
# agents/prompts/system.jinja2
"""
You are a {{ role }} assistant.
Today: {{ date }}
Available tools: {{ tool_names | join(", ") }}
"""

# agent.py
from jinja2 import Environment, FileSystemLoader

env = Environment(loader=FileSystemLoader("agents/prompts"))

def build_system_prompt(role: str, tools: list[str]) -> str:
    template = env.get_template("system.jinja2")
    return template.render(role=role, date=date.today(), tool_names=tools)
```

> [!TIP] Treat prompts like config
> Store prompt templates in files (`prompts/`), version them in git, and load them at runtime. This enables A/B testing and rollback without code changes.

---

## 2. Design Tools as Pure Functions

Each tool should be:
- **Deterministic** given the same inputs (or explicitly documented as non-deterministic)
- **Narrowly scoped** — one capability per tool
- **Independently testable** without an LLM

```python
# ❌ Tool with side effects + multiple responsibilities
def user_tool(action: str, user_id: int, data: dict = None):
    if action == "get":   return db.get_user(user_id)
    if action == "update": db.update(user_id, data); send_notification(user_id)
    if action == "delete": db.delete(user_id)

# ✅ One tool per action, explicit types, no hidden side effects
from pydantic import BaseModel

class GetUserInput(BaseModel):
    user_id: int

def get_user(input: GetUserInput) -> dict:
    """Fetch user profile by ID. Returns None if not found."""
    return db.get_user(input.user_id)

class UpdateUserInput(BaseModel):
    user_id: int
    fields: dict[str, str]

def update_user(input: UpdateUserInput) -> dict:
    """Update user fields. Returns updated profile."""
    return db.update_user(input.user_id, input.fields)
```

### Tool Schema Golden Rules

- **Explicit types** — use Pydantic models, not raw `dict`
- **Docstring is the tool description** — the LLM reads it; make it precise
- **No overloaded tools** — don't use an `action` flag to multiplex behavior
- **Validate inputs** inside the tool — never trust LLM-generated parameters blindly

---

## 3. Structured Output Parsing

Never parse free-text LLM output with string manipulation. Use structured outputs or validated schemas.

```python
# ❌ Fragile string parsing
response_text = llm.call(prompt)
if "Action:" in response_text:
    action = response_text.split("Action:")[1].split("\n")[0].strip()

# ✅ Pydantic + instructor / structured output mode
from pydantic import BaseModel
import instructor
from openai import OpenAI

class AgentDecision(BaseModel):
    thought: str
    action: Literal["search", "summarize", "respond", "hand_off"]
    action_input: str
    confidence: float

client = instructor.from_openai(OpenAI())

def get_next_action(context: str) -> AgentDecision:
    return client.chat.completions.create(
        model="gpt-4o",
        response_model=AgentDecision,
        messages=[{"role": "user", "content": context}],
    )
```

---

## 4. Explicit Control Flow — Avoid Implicit Agentic Loops

**Anti-pattern**: deep framework magic where you can't tell what happens next.

```python
# ❌ Framework runs the whole loop — no visibility or control
agent.run("Do whatever you think is best")

# ✅ Explicit loop — every step is observable and interruptible
MAX_STEPS = 10

for step in range(MAX_STEPS):
    decision = get_next_action(build_context(memory, history))
    logger.info("step=%d action=%s input=%s", step, decision.action, decision.action_input)

    if decision.action == "respond":
        return decision.action_input   # terminal condition

    result = execute_tool(decision.action, decision.action_input)
    history.append(ToolResult(step=step, tool=decision.action, result=result))

raise AgentLoopExceededError(f"Agent did not terminate within {MAX_STEPS} steps")
```

> [!WARNING] Always enforce a max step limit
> Without a loop guard, a misbehaving agent hallucinates tool calls indefinitely. Always raise, don't silently return.

---

## 5. Memory Management — Explicit, Not Magical

```python
# ❌ Memory that silently grows / is never audited
agent.memory.add(anything)

# ✅ Typed memory entries with expiry and source tracking
from dataclasses import dataclass, field
from datetime import datetime

@dataclass
class MemoryEntry:
    content: str
    source: Literal["user", "tool", "reflection"]
    created_at: datetime = field(default_factory=datetime.utcnow)
    ttl_seconds: int | None = None   # None = permanent

    @property
    def is_expired(self) -> bool:
        if self.ttl_seconds is None:
            return False
        return (datetime.utcnow() - self.created_at).seconds > self.ttl_seconds
```

- **Prune expired entries** before each context build
- **Tag entries by source** (user said / tool returned / agent reflected)
- **Log what was added and what was evicted** — memory bugs are the hardest to debug

---

## 6. Observability — Log Everything

LLM agent debugging is hard: the "bug" is often in a prompt/context 3 steps ago.

```python
import structlog

log = structlog.get_logger()

def execute_tool(name: str, input_data: dict) -> dict:
    log.info("tool.start", tool=name, input=input_data)
    try:
        result = TOOL_REGISTRY[name](input_data)
        log.info("tool.success", tool=name, output_preview=str(result)[:200])
        return result
    except Exception as e:
        log.error("tool.failed", tool=name, error=str(e))
        raise ToolExecutionError(f"{name} failed: {e}") from e
```

**Minimum log events per agent run:**

| Event | Data to log |
|---|---|
| `agent.start` | session_id, user_input, model, tools available |
| `step.decision` | step #, action chosen, confidence, thought |
| `tool.start / tool.end` | tool name, input params, output size |
| `memory.add / memory.evict` | entry content hash, source, reason |
| `agent.end` | final output, total steps, total tokens, latency |

---

## 7. Error Handling for Agents

```python
# Custom exception hierarchy
class AgentError(Exception): ...
class ToolExecutionError(AgentError): ...
class OutputParsingError(AgentError): ...
class AgentLoopExceededError(AgentError): ...
class ContextWindowExceededError(AgentError): ...

# Retry pattern for transient LLM/tool failures
from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type

@retry(
    retry=retry_if_exception_type((RateLimitError, APITimeoutError)),
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10),
)
def call_llm(messages: list[dict]) -> str: ...
```

---

## 8. Testing Agents

Test each layer in isolation:

| Layer | Test approach |
|---|---|
| **Prompt template** | Unit test rendered output for edge-case inputs |
| **Tool function** | Standard unit tests; no LLM needed |
| **Output parser** | Assert parsing handles valid and malformed LLM outputs |
| **Control flow** | Mock LLM responses; assert correct tool was called in order |
| **Full agent** | Integration test with a real (cheap) model and recorded fixtures |

```python
# Mock LLM for control flow test
def test_agent_calls_search_then_responds(mock_llm):
    mock_llm.side_effect = [
        AgentDecision(action="search", action_input="latest news", thought="...", confidence=0.9),
        AgentDecision(action="respond", action_input="Here's the news...", thought="...", confidence=0.95),
    ]
    result = run_agent("What's the latest news?")
    assert mock_llm.call_count == 2
    assert result == "Here's the news..."
```

---

## Common Anti-Patterns Summary

| Anti-Pattern | Risk | Fix |
|---|---|---|
| Prompt strings in Python code | Hard to maintain, no versioning | Externalize to template files |
| Multi-action tools with `action` flag | Confused LLM, hard to test | One tool = one action |
| No loop termination guard | Infinite hallucination loop | `MAX_STEPS` + `AgentLoopExceededError` |
| Free-text output parsing | Brittle; breaks on format changes | Structured output + Pydantic |
| Silent memory growth | Context overflow, stale data | TTL + explicit eviction logging |
| Bare `except Exception` in tools | Swallows diagnostic info | Log + wrap in domain exception |
| No session ID | Impossible to correlate logs | Generate UUID at session start |

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.02 LLM Agents]] - Agent patterns and architectures
- [[11.03 LLM Tool Calls]] - Tool call mechanics
- [[11.04 LLM Workflows]] - Orchestration patterns
- [[11.20 LLM Memory Architecture]] - Memory types for agents
- [[11.14 Prompt Engineering]] - Prompt design principles
- [[71.06 Python Clean Code]] - General Python clean code principles
- [[71.07 SOLID Principles]] - Design principles for maintainable agent components
