---
id: 11.21
tags: [type/tool, status/growing, context/llm]
---

# Cerebras

## Overview
**Cerebras** is an AI infrastructure company that builds the **Wafer-Scale Engine (WSE)**, the world's largest processor, purpose-built for ultra-fast AI training and inference. Cerebras offers cloud, dedicated, and on-prem deployment options with OpenAI-compatible APIs.

## Key Concepts

- **Wafer-Scale Engine (WSE)** — A single chip spanning an entire silicon wafer (~46,225 mm²), orders of magnitude larger than any GPU
- **On-chip memory** — Massive SRAM eliminates off-chip memory bottleneck
- **No multi-chip communication overhead** — Everything runs on one wafer, avoiding interconnect latency
- **Full-parameter models** — Runs models at full precision without compromises on size or quality
- **Train + Infer platform** — Single platform for pre-training, fine-tuning, and serving

## WSE vs GPU for Inference

| Aspect | GPU (e.g., NVIDIA H100) | Cerebras WSE |
|---|---|---|
| **Chip size** | ~814 mm² | ~46,225 mm² (full wafer) |
| **On-chip SRAM** | ~50 MB | ~40+ GB |
| **Design** | Multi-chip clusters via interconnect | Single wafer, no inter-chip communication |
| **Scaling** | Network-bound (NVLink, InfiniBand) | Wafer-internal fabric |
| **Inference speed** | Hundreds of tokens/s | Thousands of tokens/s (2,000+ tok/s reported) |
| **Training support** | Full support | Full support (pre-train + fine-tune) |

## Why It's Fast

> [!INFO] Eliminating Data Movement
> Like [[11.20 Groq]], Cerebras addresses the [[11.18 LLM Throughput & Memory Bound|memory-bound bottleneck]] — but with a different approach: instead of small custom chips with SRAM, Cerebras uses one massive wafer with enough on-chip SRAM to hold entire model weights, eliminating off-chip data movement entirely.

- **All-on-chip compute** — Model weights stay on-wafer, no HBM round-trips
- **Massive parallelism** — Hundreds of thousands of cores on a single wafer
- **No interconnect tax** — No multi-GPU communication overhead for large models
- **Up to 15x faster inference** than GPU clouds (per Cerebras benchmarks)

## Cerebras Cloud Platform

- **OpenAI-compatible API** — Drop-in replacement for existing integrations
- **Supported models** — GLM, OpenAI open models (GPT-OSS), Qwen, Llama, and more
- **Deployment options:**

| Mode | Description |
|---|---|
| **Cloud** | Serve open models via API key |
| **Dedicated** | Private cloud API/endpoint with dedicated capacity |
| **On-prem** | Full control of models, data, and infrastructure |

- **Certifications** — SOC2, HIPAA compliant
- **Training support** — Fine-tune or pre-train custom models on the same platform

## Practical Use Cases

- **Real-time search & copilots** — Complex reasoning in under a second (AlphaSense, Notion)
- **Multi-step agents** — Execute workflows without delays or timeouts (NinjaTech)
- **Code generation** — Instant code, debug, refactor at speed of thought (Cognition/Devin)
- **Voice AI** — Ultra-low latency for natural conversations (Tavus, LiveKit)
- **Drug discovery** — Hundreds of times faster model execution for pharmaceutical research (GSK, Argonne)

## Notable Partnerships
- **OpenAI** — Dedicated low-latency inference solution for ChatGPT platform
- **Meta** — Serving Llama Scout at 2,000+ tokens/s (30x faster than closed models)
- **Mayo Clinic** — Genomic data analysis for treatment decisions

## Related Concepts
- [[11_LLM_Dev_MOC]] — Parent MOC
- [[11.18 LLM Throughput & Memory Bound]] — The memory-bound problem that WSE addresses
- [[11.17 Mixture of Experts]] — Large model architectures Cerebras serves efficiently
- [[11.20 Groq]] — Competing custom-silicon inference provider (LPU-based)
