---
id: 11.15
tags: [type/concept, status/evergreen, context/llm]
---

# LLM Evaluation Metrics

## Overview
**LLM Evaluation Metrics** are quantitative and qualitative measures used to assess the performance, quality, and safety of large language model outputs. Evaluation is split into **automated metrics** (scalable, reproducible) and **human/LLM-as-judge** approaches (nuanced, costly).

## Key Concepts
- **Reference-based metrics**: Compare output against a ground-truth answer (e.g., BLEU, ROUGE)
- **Reference-free metrics**: Evaluate output quality without ground truth (e.g., LLM-as-Judge, perplexity)
- **Task-specific vs general**: Some metrics only apply to certain tasks (e.g., code pass@k, summarization ROUGE)
- **Multi-dimensional evaluation**: A single metric rarely captures full quality — combine **correctness**, **fluency**, **faithfulness**, **safety**

## Automated Metrics

| Metric | Type | What It Measures | Best For |
|---|---|---|---|
| **BLEU** | N-gram overlap | Precision of generated n-grams vs reference | Translation |
| **ROUGE** (1/2/L) | N-gram overlap | Recall of reference n-grams in output | Summarization |
| **BERTScore** | Embedding similarity | Semantic similarity via contextual embeddings | General text quality |
| **Perplexity** | Probabilistic | How "surprised" the model is by text | Language modeling, fluency |
| **Pass@k** | Functional correctness | % of code samples passing unit tests (k attempts) | Code generation |
| **Exact Match (EM)** | String match | Whether output exactly matches reference | QA, extraction |
| **F1 Score** | Token overlap | Token-level precision/recall vs reference | QA, NER |

## RAG-Specific Metrics

| Metric | Evaluates | Description |
|---|---|---|
| **Faithfulness** | Generation | Does the answer stay grounded in retrieved context? |
| **Answer Relevancy** | Generation | Does the answer address the question? |
| **Context Precision** | Retrieval | Are retrieved docs relevant and ranked well? |
| **Context Recall** | Retrieval | Are all needed docs retrieved? |

> [!TIP] RAG evaluation frameworks
> Tools like **Ragas**, **DeepEval**, and **LangSmith** provide built-in RAG metric pipelines.

## LLM-as-Judge

Uses a strong LLM (e.g., GPT-4) to evaluate outputs on criteria like helpfulness, harmlessness, and honesty.

- **Pointwise**: Rate a single output on a rubric (1–5 scale)
- **Pairwise**: Compare two outputs and pick the better one
- **Reference-guided**: Judge scores against a gold answer

| Pros | Cons |
|---|---|
| Scales better than human eval | Positional bias (prefers first/last response) |
| Captures nuance automated metrics miss | Self-preference bias (favors own model family) |
| Customizable rubrics | Verbosity bias (prefers longer answers) |

> [!WARNING] Known biases in LLM-as-Judge
> Mitigate with: **randomized ordering**, **multiple judges**, **structured rubrics**, and **calibration examples**.

## Evaluation Strategy

```
Offline Eval ──► A/B Testing ──► Production Monitoring
(benchmarks)     (user-facing)    (drift, quality)
```

- **Offline**: Benchmark suites (MMLU, HumanEval, MT-Bench) before deployment
- **Online A/B**: Compare model versions with real users
- **Production**: Track metrics continuously via [[24.01 Model Drift Detection]]

## Practical Use Cases
- **Model selection** — Compare base models on task-specific benchmarks
- **Prompt iteration** — Measure improvement across prompt versions
- **Fine-tuning validation** — Ensure fine-tuned model doesn't regress
- **RAG pipeline tuning** — Balance retrieval recall vs generation faithfulness
- **Safety auditing** — Evaluate toxicity, bias, and refusal rates

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.14 Prompt Engineering]] - Evaluate prompt effectiveness
- [[11.12 RAG]] - Evaluate retrieval quality (faithfulness, context metrics)
- [[11.13 Chain of Thought]] - Evaluate reasoning step quality
- [[24.01 Model Drift Detection]] - Monitor metrics over time in production
