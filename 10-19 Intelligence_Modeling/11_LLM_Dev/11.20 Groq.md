---
id: 11.20
tags: [type/tool, status/growing, context/llm]
---

# Groq

## Overview
**Groq** is an AI inference company that designs the **LPU (Language Processing Unit)**, a custom chip purpose-built for fast, low-cost LLM inference. Founded in 2016, Groq offers cloud-based inference via **GroqCloud** with OpenAI-compatible APIs.

## Key Concepts

- **LPU (Language Processing Unit)** — Custom ASIC designed specifically for inference workloads, not adapted from GPUs
- **Deterministic execution** — Static scheduling via a custom compiler ensures predictable latency at every scale
- **Single-core architecture** — Software-defined single-core design removes traditional hardware complexity
- **On-chip SRAM** — Hundreds of MB of SRAM as primary weight storage (not cache), eliminating HBM bandwidth bottlenecks
- **Chip-to-chip connectivity** — Plesiosynchronous protocol aligns hundreds of chips to act as a single logical core

## LPU vs GPU for Inference

| Aspect | GPU (e.g., NVIDIA A100/H100) | Groq LPU |
|---|---|---|
| **Design purpose** | General-purpose parallel compute | Inference-specific |
| **Memory** | HBM (high bandwidth, off-chip) | On-chip SRAM (lower latency) |
| **Scheduling** | Dynamic (runtime) | Static (compile-time, deterministic) |
| **Bottleneck** | Memory-bound during decode | Mitigated by on-chip SRAM |
| **Cooling** | Liquid cooling common | Air-cooled by design |
| **Strength** | Training + inference flexibility | Ultra-low latency inference |

## Why It's Fast

> [!INFO] Solving the Memory-Bound Problem
> Traditional GPUs are [[11.18 LLM Throughput & Memory Bound|memory-bound during decode]] — they must load entire model weights from HBM for each token. The LPU stores weights in on-chip SRAM, drastically reducing data movement latency.

- **No cache hierarchy** — Weights live on-chip, no cache misses
- **Compiler-predicted data movement** — The compiler schedules both compute and network, no runtime overhead
- **Continuous token-based execution** — Every cycle is accounted for, no wasted operations

## GroqCloud Platform

- **OpenAI-compatible API** — Drop-in replacement, just change `base_url`
- **Supported models** — Open models including Llama, Mixtral, Gemma, and others
- **Free tier** — Available for developers to start building
- **Global data centers** — Low-latency inference worldwide

```python
import openai

client = openai.OpenAI(
    base_url="https://api.groq.com/openai/v1",
    api_key="GROQ_API_KEY"
)
```

## Practical Use Cases

- **Real-time AI applications** — Sub-second responses for copilots, agents, chat
- **Cost-sensitive inference** — Reported 89% cost reduction in customer cases
- **Latency-critical workloads** — Voice AI, live analysis, interactive coding
- **MoE model serving** — Optimized for Mixture of Experts architectures

## Related Concepts
- [[11_LLM_Dev_MOC]] — Parent MOC
- [[11.18 LLM Throughput & Memory Bound]] — The memory-bound problem that LPU addresses
- [[11.17 Mixture of Experts]] — MoE models Groq is optimized for
- [[11.21 Cerebras]] — Competing custom-silicon inference provider (wafer-scale)
