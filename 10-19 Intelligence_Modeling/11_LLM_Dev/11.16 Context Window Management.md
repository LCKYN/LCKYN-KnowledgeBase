---
id: 11.16
tags: [type/concept, status/growing, context/llm]
---

# Context Window Management

## Overview
**Context window management** is the discipline of deciding **what goes inside the context window** at each inference call. Since the context window is the model's only "live" working memory — and every token costs compute — managing it well is central to building cost-effective, high-quality LLM systems.

---

## The Context Window Budget

Every model has a fixed token limit. The budget must be shared across all content types:

```
[ System Prompt ] [ Few-shot Examples ] [ Retrieved Docs ] [ History ] [ Current Turn ] [ Output ]
     ↑ Static              ↑ Semi-static        ↑ Dynamic       ↑ Grows   ↑ User input    ↑ Reserved
```

| Slot | Typical token budget | Notes |
|---|---|---|
| System prompt | 500–2,000 | Keep concise; prime candidate for [[11.21 Prompt Caching]] |
| Few-shot examples | 500–3,000 | Cache these; only include task-relevant examples |
| Retrieved docs (RAG) | 2,000–20,000 | Most variable; rank by relevance, trim aggressively |
| Conversation history | 1,000–10,000 | Grows unboundedly — needs active management |
| Current user turn | 50–500 | Usually small |
| Reserved for output | 500–4,000 | Must leave room or truncation occurs |

---

## Management Strategies

### 1. Sliding Window
Keep only the **last N turns** of conversation. Simple but loses early context.
```
History: [turn-8, turn-9, turn-10, current]  ← turn-1..7 dropped
```

### 2. Summarization / Compression
Periodically **summarize** old turns into a compact paragraph and replace the raw history:
```
[Summary of turns 1–8: "User is building a RAG pipeline for legal docs..."] + [turn-9, turn-10, current]
```

### 3. Retrieval-Augmented History
Store old turns in a vector DB; **retrieve only relevant ones** for the current query — same pattern as [[11.12 RAG]] but for conversation history.

### 4. Priority Eviction
Assign importance scores to context slots; evict lowest-priority content first when budget is tight:
```
Priority: System prompt > Current turn > Critical tool results > Recent history > Old history
```

### 5. Prompt Caching
Make static content (system prompt, docs, few-shots) a **cached prefix** so reusing it costs ~10% of normal token price. See [[11.21 Prompt Caching]].

### 6. Structured Context (KV Injection)
Instead of free-form history, maintain a structured state object that gets serialized into the prompt — easier to prune individual fields.

---

## Context Window Sizes (2024–2026)

| Model | Context window |
|---|---|
| GPT-4o | 128K |
| Claude 3.5 Sonnet | 200K |
| Gemini 1.5 Pro | 1M |
| Gemini 2.0 Flash | 1M |
| Llama 3.1 405B | 128K |

> [!WARNING] Long context ≠ free
> Larger context windows don't eliminate the need for management — prefill cost scales with context length, and model attention quality can degrade on very long contexts ("lost in the middle" problem).

---

## Practical Use Cases

- **Chatbots**: Summarize history every 10 turns to stay under budget
- **Code agents**: Keep current file + relevant functions; evict unrelated files
- **RAG pipelines**: Top-k retrieval with token-budget-aware chunk selection
- **Document Q&A**: Cache the document; stream different questions against same prefix

---

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.20 LLM Memory Architecture]] - Full memory taxonomy (working, archival, entity)
- [[11.21 Prompt Caching]] - Cache static prefix to reduce cost
- [[11.12 RAG]] - Selective context retrieval from archival memory
- [[11.14 Prompt Engineering]] - Structure prompts to fit within budget
- [[11.18 LLM Throughput & Memory Bound]] - Token cost and KV cache memory pressure
