---
id: 11.17
tags: [type/concept, status/seed, context/llm]
---

# Mixture of Experts (MoE)

## Overview
**Mixture of Experts (MoE)** is a neural network architecture that uses a **gating mechanism** to route each input to a subset of specialized sub-networks (**experts**), enabling massive model capacity without proportionally increasing compute cost per inference.

## Key Concepts
- **Experts**: Independent feed-forward networks, each specializing in different input patterns
- **Gating Network (Router)**: Learned module that decides which experts process each token
- **Sparse Activation**: Only a few experts (e.g., 2 out of 64) are activated per token → compute stays manageable
- **Top-k Routing**: Router selects top-k experts per token; outputs are weighted-summed
- **Total Parameters vs Active Parameters**: MoE models have large total params but only a fraction is active per forward pass

## How It Works

```
Input Token → Router → Top-k Expert Selection → Weighted Sum of Expert Outputs → Output
```

| Component | Role |
|---|---|
| Router / Gate | Produces probability distribution over experts |
| Expert FFN | Processes token independently |
| Load Balancing Loss | Auxiliary loss to prevent routing collapse (all tokens → same expert) |

## Notable MoE Models

| Model | Total Params | Active Params | Experts | Top-k |
|---|---|---|---|---|
| Mixtral 8x7B | ~47B | ~13B | 8 | 2 |
| GPT-4 (rumored) | ~1.8T | ~280B | 16 | 2 |
| Switch Transformer | varies | varies | up to 2048 | 1 |
| DeepSeek-V2 | 236B | 21B | 160 | 6 |

## Advantages
- **Scales capacity** without linear compute increase
- **Faster inference** than equivalent dense model
- **Specialization** — experts learn distinct patterns

## Challenges
- **Load balancing** — uneven routing degrades performance
- **Memory footprint** — all experts must be loaded even if sparse
- **Training instability** — routing can collapse; requires auxiliary losses
- **Communication overhead** — in distributed setups, expert parallelism adds latency

> [!TIP] When to prefer MoE
> Use MoE when you need **large model capacity** but are constrained on **inference compute budget**. Ideal for serving at scale where latency matters.

> [!WARNING] Memory vs Compute tradeoff
> MoE saves compute but **not memory** — all expert weights must reside in memory (or be offloaded), making deployment on small GPUs challenging.

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.01 Attention Mechanism]] - Core building block of transformer experts
- [[11.16 Context Window Management]] - MoE affects per-token compute budget
- [[13.10 Activation Functions]] - Used within expert FFN layers
