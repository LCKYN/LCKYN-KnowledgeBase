---
id: 11.03
tags: [type/concept, status/growing, context/llm]
---

# LLM Tool Calls

## Overview
Tool calling (also called function calling) enables LLMs to interact with external systems by generating structured outputs that trigger specific functions or APIs. This bridges the gap between language understanding and actionable operations.

## How It Works

### 1. Tool Definition
Define available tools with schemas:
```json
{
  "name": "get_weather",
  "description": "Get current weather for a location",
  "parameters": {
    "type": "object",
    "properties": {
      "location": {"type": "string"},
      "unit": {"type": "string", "enum": ["celsius", "fahrenheit"]}
    },
    "required": ["location"]
  }
}
```

### 2. LLM Decision
Model analyzes user query and decides:
- Which tool to call (if any)
- What parameters to extract
- Returns structured tool call request

### 3. Execution
Application layer:
- Validates tool call
- Executes actual function
- Returns results to LLM
- LLM formulates final response

## Provider Implementations

### OpenAI Function Calling
```python
messages = [{"role": "user", "content": "What's the weather in Paris?"}]
response = client.chat.completions.create(
    model="gpt-4",
    messages=messages,
    tools=tools,
    tool_choice="auto"
)
tool_call = response.choices[0].message.tool_calls[0]
```

### Anthropic Tool Use
Uses XML-like format in prompts:
```xml
<function_calls>
<invoke name="get_weather">
<parameter name="location">Paris</parameter>
</invoke>
</function_calls>
```

### Google/Vertex AI
Function declarations in system instructions

## Tool Design Patterns

### 1. Single Tool Call
One function per turn
- Simple, predictable
- Good for specific tasks

### 2. Parallel Tool Calls
Multiple simultaneous calls
- Efficient for independent operations
- Reduces latency

### 3. Sequential Tool Calls
Chain multiple calls
- Each call informs the next
- Building complex workflows

### 4. Nested/Recursive Calls
Tools that trigger other tools
- Agent-like behavior
- Requires careful control

## Best Practices

### Tool Schema Design
- **Clear descriptions**: Help model understand when to use
- **Explicit parameters**: Type hints, constraints, examples
- **Minimal ambiguity**: Reduce confusion between similar tools
- **Granular functions**: Smaller, focused tools over monolithic ones

### Error Handling
- Validate parameters before execution
- Return clear error messages to LLM
- Implement retry logic
- Set timeouts and rate limits

### Security
- Whitelist allowed functions
- Validate and sanitize inputs
- Use least privilege for tool execution
- Audit and log all tool calls

## Common Tool Categories

### Information Retrieval
- Search engines
- Database queries
- Vector store retrieval
- Web scraping

### Computation
- Calculator
- Code interpreter
- Data analysis
- Unit conversion

### External Services
- Email/messaging
- Calendar management
- File operations
- API integrations

### State Management
- Read/write memory
- Session storage
- Context management

## Frameworks Support

| Framework | Tool Calling Support |
|-----------|---------------------|
| LangChain | ✅ Built-in tools + custom |
| LlamaIndex | ✅ Function calling agents |
| Haystack | ✅ Tool nodes |
| Semantic Kernel | ✅ Native functions |

## Challenges
- **Hallucinated calls**: Model invents non-existent tools
- **Parameter errors**: Wrong types or invalid values
- **Over-calling**: Excessive tool use when unnecessary
- **Cost**: Each call adds tokens and latency

## Optimization Strategies
1. Few-shot examples of correct tool usage
2. Constrain tool choice when context is clear
3. Cache tool results when applicable
4. Batch compatible operations
5. Monitor and optimize tool descriptions

## Related Concepts
- [[11.02 LLM Agents]]
- [[11.04 LLM Workflows]]

## References
- OpenAI Function Calling Guide
- Anthropic Tool Use Documentation
- Gorilla: Large Language Model Connected with Massive APIs
