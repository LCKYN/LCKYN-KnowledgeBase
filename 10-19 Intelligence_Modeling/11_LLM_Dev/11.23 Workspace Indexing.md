---
id: 11.23
tags: [type/concept, status/evergreen, context/llm]
---

# Workspace Indexing for LLM

## Overview
**Workspace indexing** is the process of parsing, chunking, embedding, and storing a codebase (or document corpus) so that an LLM-based assistant can retrieve only the **relevant context** for a given query — working around the finite context window. It is essentially [[11.12 RAG]] applied specifically to code and project files.

## Why It's Needed

```
Full codebase: 500K–10M tokens
Context window: 8K–200K tokens
→ Must select the right ~1–5% to send to the LLM
```

Without indexing, assistants either:
- Truncate context (miss relevant files)
- Send everything (slow, expensive, hits limit)
- Ask the user to manually paste files (bad UX)

## Indexing Pipeline

```
Files on disk
    │
    ▼
1. Discovery        → enumerate files, respect .gitignore, filter by extension
    │
    ▼
2. Parsing          → language-aware AST parsing (tree-sitter) or plain text
    │
    ▼
3. Chunking         → split into retrieval units (functions, classes, lines)
    │
    ▼
4. Embedding        → encode each chunk → dense vector (1536-dim etc.)
    │
    ▼
5. Vector Store     → persist vectors + metadata (file, line range, symbol name)
    │
    ▼
6. Retrieval        → at query time: embed query → ANN search → top-k chunks
    │
    ▼
7. Context Assembly → rank, deduplicate, format chunks → inject into LLM prompt
```

## Chunking Strategies

| Strategy | How | Pros | Cons |
|---|---|---|---|
| **Fixed-size (sliding window)** | Split every N tokens/chars with overlap | Simple, language-agnostic | Cuts mid-function |
| **Line-based** | Split on newlines (N lines per chunk) | Preserves code lines | Still splits logical units |
| **AST/Syntax-aware** | Split on function/class boundaries via tree-sitter | Semantically coherent | Requires language-specific parser |
| **Semantic chunking** | Embed sentences, split where cosine similarity drops | Best semantic boundaries | Slow; requires embedding pass first |
| **Symbol-level** | One chunk per function/method/class | Ideal for code; enables symbol lookup | Requires full parse |

> [!TIP] Best practice for code
> Use **AST/symbol-level** chunking: one chunk = one function or class. Attach metadata: file path, language, docstring, outgoing call references.

## Indexing Types

| Type | What It Stores | Retrieval Signal |
|---|---|---|
| **Flat semantic index** | Embedded chunks from files | Cosine similarity to query |
| **Symbol / tag index** | Function/class names, signatures (like ctags) | Exact or fuzzy name match |
| **Call graph index** | Who calls what | Expand context along call chains |
| **Dependency graph** | Import relationships between modules | Find all files affected by a change |
| **Hybrid** | Semantic + symbol + graph | Best coverage; used by Sourcegraph Cody |

## Retrieval Methods

| Method | Technique | Good For |
|---|---|---|
| **Semantic (dense)** | Embed query → cosine ANN over vector store | "How does auth work?" style questions |
| **Keyword (sparse)** | BM25 / TF-IDF over token index | Exact identifiers, variable names |
| **Hybrid** | Reciprocal Rank Fusion (RRF) of dense + sparse | Best overall recall |
| **Graph traversal** | Follow call/import edges from seed symbols | "What does this function affect?" |
| **Exact lookup** | File path or symbol name → direct fetch | "Show me `UserService.create`" |

## Incremental Indexing

Re-indexing the entire workspace on every change is too slow. Efficient systems:

- Track **file modification timestamps** or **git diff**
- Only re-chunk and re-embed **changed files**
- Use **content hash** of each chunk to detect changes at chunk level
- Background indexing thread — index update doesn't block the user

```
on file_save(path):
  if hash(file) != stored_hash(path):
    chunks = parse_and_chunk(path)
    vectors = embed(chunks)
    vector_store.upsert(path, vectors)
    stored_hash[path] = hash(file)
```

## Tools & Parsers

| Tool | Role |
|---|---|
| **tree-sitter** | Fast, language-agnostic AST parser (50+ languages); ideal for symbol-level chunking |
| **ctags / universal-ctags** | Lightweight symbol extraction; used by Aider's repo-map |
| **Chroma / Qdrant / FAISS** | Local vector stores for embeddings |
| **text-embedding-3-small** (OpenAI) | High-quality code embeddings (1536-dim) |
| **nomic-embed-text** | Open-source alternative; good for code |
| **LanceDB** | Embedded vector DB; no server required; good for local tools |

## How Coding Assistants Do It

| Tool | Indexing Approach |
|---|---|
| **GitHub Copilot** | Embedding-based index of open files + workspace; local index synced in background |
| **Cursor** | **Repo-map** (ctags-style symbol tree) + semantic embedding index; `@codebase` search |
| **Aider** | **Repo-map**: ranked graph of symbols + call relationships; fits in ~1K tokens |
| **Sourcegraph Cody** | **Precise code intelligence** (SCIP/LSIF call graph) + semantic embeddings |
| **Continue.dev** | Configurable: local embeddings + BM25; pluggable vector store |
| **Copilot Workspace** | Full-repo context + plan-and-edit agent; no retrieval-only model |

### Aider Repo-Map (notable technique)

Aider builds a **compressed symbol map** of the entire repo that fits in ~1K tokens:

```
repo-map/
  src/auth.py: AuthService.create(), AuthService.verify(), TokenStore
  src/db.py:   Database.connect(), query(), UserTable
  src/api.py:  routes: /login, /logout, /refresh → calls AuthService
```

- Uses **PageRank on the symbol call graph** to rank the most important symbols
- Expands detail for symbols relevant to the current task
- No embedding needed — pure structural analysis

## Context Assembly Best Practices

> [!TIP] What to include in the assembled context
> 1. **Exact match hits** — file/symbol the user directly named
> 2. **Semantic top-k** — highest cosine similarity chunks (typically 5–10)
> 3. **Call context** — callers/callees of the focal function (1 hop)
> 4. **Open editor files** — always include currently open file in full
> 5. **Recent git diff** — recent changes for debugging tasks

> [!WARNING] Avoid
> - Sending duplicate chunks (deduplicate by content hash)
> - Including auto-generated files (`node_modules`, `dist`, `.lock`)
> - Stale index: changes since last index confuse the LLM

## Related Concepts
- [[11_LLM_Dev_MOC]] - Parent category
- [[11.12 RAG]] - Workspace indexing is RAG applied to code/documents
- [[11.16 Context Window Management]] - Indexing solves the context limit problem
- [[11.14 Prompt Engineering]] - Context assembly is a form of prompt construction
- [[11.02 LLM Agents]] - Agents use workspace indexing to navigate large codebases
- [[11.08 MCP Server]] - MCP tools can expose workspace index as callable context retrieval
