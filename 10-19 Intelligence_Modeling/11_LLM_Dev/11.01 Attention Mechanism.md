---
id: 11.01
tags: [type/concept, status/seed, context/llm]
---

# Attention Mechanism

## Definition
**Attention** is a mechanism that allows neural networks to focus on specific parts of the input sequence when processing data, rather than treating the entire input as a fixed-size vector. It mimics cognitive attention.

## Key Concepts
- **Query, Key, Value (Q, K, V)**: The core components of attention.
    - **Query**: What I am looking for.
    - **Key**: What I can offer.
    - **Value**: What I actually contain.
- **Self-Attention**: Relates different positions of a single sequence to compute a representation of the sequence.
- **Multi-Head Attention**: Runs multiple attention mechanisms in parallel to capture different types of relationships.

## Importance
- **Solves Long-Range Dependencies**: Unlike RNNs/LSTMs, attention connects distant words directly.
- **Parallelization**: Enables parallel processing of sequences (unlike sequential RNNs), leading to the **Transformer** architecture.
