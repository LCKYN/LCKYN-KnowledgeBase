---
id: 12.10
tags: [type/concept, status/evergreen, context/mle, context/mlops]
---

# MLflow

## Overview
MLflow is an open-source platform for managing the end-to-end machine learning lifecycle. It provides tools for experiment tracking, model packaging, versioning, and deployment.

## Core Components

```mermaid
graph LR
    A[MLflow Tracking] --> B[Log Experiments]
    C[MLflow Projects] --> D[Reproducible Runs]
    E[MLflow Models] --> F[Package & Deploy]
    G[Model Registry] --> H[Version & Stage]
```

| Component | Purpose |
|-----------|---------|
| **Tracking** | Log parameters, metrics, artifacts |
| **Projects** | Package code for reproducibility |
| **Models** | Standard format for deployment |
| **Registry** | Centralized model versioning |

## Installation

```bash
pip install mlflow

# With extras
pip install mlflow[extras]  # sklearn, etc.

# Start UI
mlflow ui --port 5000
```

## Tracking Experiments

### Basic Logging

```python
import mlflow

# Start a run
with mlflow.start_run():
    # Log parameters
    mlflow.log_param("learning_rate", 0.01)
    mlflow.log_param("max_depth", 5)

    # Log metrics
    mlflow.log_metric("accuracy", 0.95)
    mlflow.log_metric("f1_score", 0.93)

    # Log metrics over time
    for epoch in range(100):
        mlflow.log_metric("loss", loss, step=epoch)

    # Log artifacts (files)
    mlflow.log_artifact("model.pkl")
    mlflow.log_artifact("confusion_matrix.png")
```

### Autologging

```python
import mlflow

# Automatic logging for supported frameworks
mlflow.sklearn.autolog()
mlflow.xgboost.autolog()
mlflow.lightgbm.autolog()
mlflow.pytorch.autolog()
mlflow.tensorflow.autolog()

# Train as usual - everything logged automatically
model = RandomForestClassifier(n_estimators=100, max_depth=5)
model.fit(X_train, y_train)
```

### Experiments & Runs

```python
import mlflow

# Set experiment (creates if not exists)
mlflow.set_experiment("my-classification-project")

# Named run
with mlflow.start_run(run_name="baseline-rf"):
    mlflow.log_params({"n_estimators": 100, "max_depth": 5})
    # ... training code ...

# Nested runs
with mlflow.start_run(run_name="hyperparameter-search"):
    for params in param_grid:
        with mlflow.start_run(nested=True):
            mlflow.log_params(params)
            # ... training code ...
```

## Tracking Server

### Local Storage
```bash
# Default: ./mlruns directory
mlflow ui
```

### Remote Tracking Server
```bash
# Start server with database backend
mlflow server \
    --backend-store-uri postgresql://user:pass@host/db \
    --default-artifact-root s3://my-bucket/mlflow \
    --host 0.0.0.0 \
    --port 5000
```

```python
# Point client to server
mlflow.set_tracking_uri("http://localhost:5000")
```

### Storage Options

| Backend | URI Format |
|---------|------------|
| Local files | `./mlruns` or `file:///path` |
| SQLite | `sqlite:///mlflow.db` |
| PostgreSQL | `postgresql://user:pass@host/db` |
| MySQL | `mysql://user:pass@host/db` |
| S3 | `s3://bucket/path` |
| GCS | `gs://bucket/path` |
| Azure Blob | `wasbs://container@account.blob.core.windows.net` |

## Model Logging & Loading

### Log Model

```python
import mlflow.sklearn

with mlflow.start_run():
    model = RandomForestClassifier()
    model.fit(X_train, y_train)

    # Log model
    mlflow.sklearn.log_model(
        model,
        artifact_path="model",
        registered_model_name="my-classifier"  # Optional: register
    )
```

### Load Model

```python
# Load from run
model = mlflow.sklearn.load_model("runs:/<run_id>/model")

# Load from registry
model = mlflow.sklearn.load_model("models:/my-classifier/Production")

# Load as generic PyFunc
model = mlflow.pyfunc.load_model("runs:/<run_id>/model")
predictions = model.predict(X_test)
```

## Model Registry

### Register Model
```python
# During logging
mlflow.sklearn.log_model(model, "model", registered_model_name="my-model")

# Or register existing run
mlflow.register_model("runs:/<run_id>/model", "my-model")
```

### Model Stages

| Stage | Purpose |
|-------|---------|
| None | Newly registered |
| Staging | Testing/validation |
| Production | Live serving |
| Archived | Deprecated |

```python
from mlflow import MlflowClient

client = MlflowClient()

# Transition stage
client.transition_model_version_stage(
    name="my-model",
    version=1,
    stage="Production"
)

# Load by stage
model = mlflow.pyfunc.load_model("models:/my-model/Production")
```

### Model Versioning
```python
client = MlflowClient()

# List versions
versions = client.search_model_versions("name='my-model'")

# Get latest version
latest = client.get_latest_versions("my-model", stages=["Production"])

# Add description
client.update_model_version(
    name="my-model",
    version=1,
    description="Baseline model with RF"
)
```

## MLflow Projects

### MLproject File
```yaml
name: my_project

conda_env: conda.yaml
# or
# docker_env:
#   image: my-image:latest

entry_points:
  main:
    parameters:
      learning_rate: {type: float, default: 0.01}
      max_depth: {type: int, default: 5}
    command: "python train.py --lr {learning_rate} --depth {max_depth}"

  validate:
    command: "python validate.py"
```

### Run Project
```bash
# Local
mlflow run . -P learning_rate=0.05

# From Git
mlflow run https://github.com/user/repo -P learning_rate=0.05

# With specific entry point
mlflow run . -e validate
```

## Integration Examples

### With Scikit-learn
```python
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

mlflow.sklearn.autolog()

with mlflow.start_run():
    model = RandomForestClassifier(n_estimators=100)
    scores = cross_val_score(model, X, y, cv=5)

    mlflow.log_metric("cv_mean", scores.mean())
    mlflow.log_metric("cv_std", scores.std())

    model.fit(X_train, y_train)
```

### With Optuna
```python
import mlflow
import optuna

def objective(trial):
    with mlflow.start_run(nested=True):
        params = {
            'max_depth': trial.suggest_int('max_depth', 3, 10),
            'learning_rate': trial.suggest_float('lr', 0.01, 0.3, log=True),
        }
        mlflow.log_params(params)

        model = XGBClassifier(**params)
        score = cross_val_score(model, X, y, cv=5).mean()

        mlflow.log_metric("cv_score", score)
        return score

with mlflow.start_run(run_name="optuna-search"):
    study = optuna.create_study(direction='maximize')
    study.optimize(objective, n_trials=50)

    mlflow.log_params(study.best_params)
    mlflow.log_metric("best_score", study.best_value)
```

### With PyTorch
```python
import mlflow
import mlflow.pytorch

mlflow.pytorch.autolog()

with mlflow.start_run():
    for epoch in range(num_epochs):
        train_loss = train_one_epoch(model, train_loader)
        val_loss = validate(model, val_loader)

        mlflow.log_metrics({
            "train_loss": train_loss,
            "val_loss": val_loss
        }, step=epoch)

    mlflow.pytorch.log_model(model, "model")
```

## Model Serving

### Local Serving
```bash
# Serve model
mlflow models serve -m "models:/my-model/Production" -p 5001

# Query endpoint
curl -X POST http://localhost:5001/invocations \
    -H "Content-Type: application/json" \
    -d '{"inputs": [[1, 2, 3, 4]]}'
```

### Docker
```bash
# Build container
mlflow models build-docker -m "models:/my-model/1" -n my-model-image

# Run container
docker run -p 5001:8080 my-model-image
```

## Best Practices

1. **Use autolog when possible**: Less boilerplate
2. **Name experiments meaningfully**: Easy to find later
3. **Log everything relevant**: Params, metrics, artifacts
4. **Use nested runs**: For hyperparameter searches
5. **Tag runs**: Add metadata for filtering
6. **Register production models**: Use Model Registry
7. **Version control MLproject**: Reproducibility

## Comparison with Alternatives

| Feature | MLflow | W&B | Neptune | DVC |
|---------|--------|-----|---------|-----|
| Open source | ✅ | Partial | Partial | ✅ |
| Self-hosted | ✅ | ✅ | ✅ | ✅ |
| Model Registry | ✅ | ✅ | ✅ | ❌ |
| Experiment Tracking | ✅ | ✅ | ✅ | ✅ |
| Data Versioning | ❌ | ❌ | ❌ | ✅ |
| Free tier | Unlimited | Limited | Limited | Unlimited |

## Related Concepts

- [[12_Machine_Learning_MOC]]
- [[12.09 Hyperparameter Tuning]] - Track tuning experiments
- [[12.08 Cross-Validation]] - Log CV results
- [[22.01 Model Registry]] - MLflow provides registry

## References
- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)
- [MLflow GitHub](https://github.com/mlflow/mlflow)
- [MLflow Quickstart](https://mlflow.org/docs/latest/quickstart.html)
