---
id: 12.05
tags: [type/concept, status/evergreen, context/mle]
---

# Feature Discretization

## Overview
Feature discretization (binning) is the process of converting continuous numerical features into discrete buckets or categories. This transforms quantitative variables into qualitative ones, often improving model interpretability and handling non-linear relationships.

## Why Discretize?

| Benefit | Description |
|---------|-------------|
| **Non-linearity** | Captures non-linear patterns in linear models |
| **Robustness** | Reduces impact of outliers |
| **Interpretability** | Easier to explain buckets than raw values |
| **Missing values** | Can treat missing as separate bin |
| **Algorithm compatibility** | Some models require categorical input |

## Binning Strategies

### 1. Equal-Width (Uniform) Binning
Divides range into K equal-sized intervals.

```python
import pandas as pd
import numpy as np

# Manual
bins = np.linspace(data.min(), data.max(), num=K+1)
pd.cut(data, bins=K)

# Sklearn
from sklearn.preprocessing import KBinsDiscretizer
kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform')
kbd.fit_transform(X)
```

| Pros | Cons |
|------|------|
| Simple, intuitive | Sensitive to outliers |
| Consistent bin width | Uneven sample distribution |

### 2. Equal-Frequency (Quantile) Binning
Each bin contains approximately the same number of samples.

```python
# Pandas
pd.qcut(data, q=5)  # 5 quantile bins

# Sklearn
kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='quantile')
```

| Pros | Cons |
|------|------|
| Balanced samples per bin | Variable bin widths |
| Robust to outliers | May split similar values |

### 3. K-Means Binning
Uses clustering to find natural groupings.

```python
kbd = KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='kmeans')
```

| Pros | Cons |
|------|------|
| Data-driven boundaries | Computationally expensive |
| Finds natural clusters | Non-deterministic |

### 4. Domain-Based Binning
Use domain knowledge for meaningful boundaries.

```python
# Age groups
bins = [0, 18, 35, 50, 65, 100]
labels = ['child', 'young_adult', 'adult', 'middle_aged', 'senior']
pd.cut(age, bins=bins, labels=labels)

# Income brackets
bins = [0, 30000, 60000, 100000, np.inf]
labels = ['low', 'medium', 'high', 'very_high']
```

## Comparison

| Strategy | Best For | Avoid When |
|----------|----------|------------|
| Equal-Width | Uniform distributions | Skewed data, outliers |
| Quantile | Skewed distributions | Need interpretable bounds |
| K-Means | Unknown structure | Reproducibility required |
| Domain | Business rules | No domain knowledge |

## Encoding After Discretization

### Ordinal Encoding
Preserves order, suitable for tree-based models.
```python
kbd = KBinsDiscretizer(encode='ordinal')
# Output: 0, 1, 2, 3, ...
```

### One-Hot Encoding
Creates binary columns, suitable for linear models.
```python
kbd = KBinsDiscretizer(encode='onehot')
# Output: [1,0,0], [0,1,0], [0,0,1], ...
```

### One-Hot Dense
Same as one-hot but returns dense array.
```python
kbd = KBinsDiscretizer(encode='onehot-dense')
```

## Choosing Number of Bins

| Method | Description |
|--------|-------------|
| **Sturges' Rule** | $K = 1 + \log_2(n)$ |
| **Square Root** | $K = \sqrt{n}$ |
| **Rice Rule** | $K = 2 \cdot n^{1/3}$ |
| **Cross-validation** | Optimize for model performance |
| **Domain knowledge** | Based on meaningful thresholds |

```python
import numpy as np
n = len(data)
sturges = int(1 + np.log2(n))
sqrt_rule = int(np.sqrt(n))
rice = int(2 * n**(1/3))
```

## Handling Edge Cases

### Outliers
```python
# Clip before binning
data_clipped = np.clip(data, percentile_1, percentile_99)
pd.cut(data_clipped, bins=5)
```

### Missing Values
```python
# Treat as separate category
data_binned = pd.cut(data, bins=5)
data_binned = data_binned.cat.add_categories(['missing'])
data_binned = data_binned.fillna('missing')
```

### Unseen Values at Inference
```python
# Use right=False to handle edge cases
pd.cut(new_data, bins=trained_bins, include_lowest=True)
```

## Full Example

```python
from sklearn.preprocessing import KBinsDiscretizer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression

# Define discretization for specific columns
preprocessor = ColumnTransformer([
    ('age_bin', KBinsDiscretizer(n_bins=5, strategy='quantile', encode='onehot'), ['age']),
    ('income_bin', KBinsDiscretizer(n_bins=4, strategy='uniform', encode='onehot'), ['income']),
], remainder='passthrough')

# Pipeline with model
pipeline = Pipeline([
    ('preprocess', preprocessor),
    ('classifier', LogisticRegression())
])

pipeline.fit(X_train, y_train)
```

## When to Discretize

✅ **Good for**:
- Linear models needing non-linearity
- Interpretability requirements
- Handling outliers gracefully
- Reducing noise in noisy features

❌ **Avoid when**:
- Using tree-based models (they discretize internally)
- Precise values matter
- Very few samples (lose information)
- Target leakage risk from data-driven bins

## Related Concepts
- [[12.03 Supervised Learning]]
- [[12.04 Logistic Regression]] - Benefits from discretized features
- [[12.06 One-Hot Encoding]] - Common encoding after discretization
- Feature Engineering

## References
- Scikit-learn KBinsDiscretizer Documentation
- "Feature Engineering for Machine Learning" (Zheng & Casari)
- Pandas cut/qcut Documentation
