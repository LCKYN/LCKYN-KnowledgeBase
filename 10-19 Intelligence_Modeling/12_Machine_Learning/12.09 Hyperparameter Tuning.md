---
id: 12.09
tags: [type/concept, status/evergreen, context/mle]
---

# Hyperparameters & Hyperparameter Tuning

## Overview
Hyperparameters are configuration settings external to the model that cannot be learned from data. Hyperparameter tuning (optimization) is the process of finding the best hyperparameter values to maximize model performance.

## Parameters vs Hyperparameters

| Aspect | Parameters | Hyperparameters |
|--------|------------|-----------------|
| Learned from | Training data | Set before training |
| Examples | Weights, biases | Learning rate, tree depth |
| Optimization | Gradient descent, etc. | Grid search, Bayesian, etc. |
| Location | Inside model | Outside model |

## Common Hyperparameters by Model

### Linear Models
| Hyperparameter | Description | Typical Range |
|----------------|-------------|---------------|
| `C` (regularization) | Inverse of Î» | 0.001 - 1000 |
| `penalty` | L1, L2, ElasticNet | Categorical |
| `solver` | Optimization algorithm | Categorical |

### Tree-Based Models
| Hyperparameter | Description | Typical Range |
|----------------|-------------|---------------|
| `max_depth` | Maximum tree depth | 3 - 20 |
| `min_samples_split` | Min samples to split | 2 - 50 |
| `min_samples_leaf` | Min samples in leaf | 1 - 20 |
| `n_estimators` | Number of trees | 50 - 1000 |
| `learning_rate` | Boosting step size | 0.01 - 0.3 |

### Neural Networks
| Hyperparameter | Description | Typical Range |
|----------------|-------------|---------------|
| `learning_rate` | Step size | 1e-5 - 1e-1 |
| `batch_size` | Samples per update | 16 - 512 |
| `epochs` | Training iterations | 10 - 1000 |
| `hidden_layers` | Layer count | 1 - 10 |
| `neurons` | Units per layer | 32 - 1024 |
| `dropout` | Regularization rate | 0.1 - 0.5 |

### SVM
| Hyperparameter | Description | Typical Range |
|----------------|-------------|---------------|
| `C` | Regularization | 0.1 - 100 |
| `kernel` | Kernel function | rbf, linear, poly |
| `gamma` | Kernel coefficient | scale, auto, 0.001-1 |

## Tuning Strategies

### 1. Grid Search
Exhaustive search over specified parameter grid.

```python
from sklearn.model_selection import GridSearchCV

param_grid = {
    'max_depth': [3, 5, 7, 10],
    'n_estimators': [100, 200, 500],
    'learning_rate': [0.01, 0.1, 0.2]
}

grid_search = GridSearchCV(
    estimator=XGBClassifier(),
    param_grid=param_grid,
    cv=5,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)
print(f"Best params: {grid_search.best_params_}")
print(f"Best score: {grid_search.best_score_:.4f}")
```

| Pros | Cons |
|------|------|
| Exhaustive, finds best in grid | Exponential time complexity |
| Simple to understand | Misses values between grid points |
| Parallelizable | Inefficient for many hyperparameters |

### 2. Random Search
Random sampling from parameter distributions.

```python
from sklearn.model_selection import RandomizedSearchCV
from scipy.stats import uniform, randint

param_distributions = {
    'max_depth': randint(3, 15),
    'n_estimators': randint(50, 500),
    'learning_rate': uniform(0.01, 0.3),
    'subsample': uniform(0.6, 0.4)
}

random_search = RandomizedSearchCV(
    estimator=XGBClassifier(),
    param_distributions=param_distributions,
    n_iter=100,          # Number of random combinations
    cv=5,
    scoring='f1',
    n_jobs=-1,
    random_state=42
)

random_search.fit(X_train, y_train)
```

| Pros | Cons |
|------|------|
| More efficient than grid search | May miss optimal values |
| Better coverage of search space | Still random, not directed |
| Fixed budget (n_iter) | No learning from previous trials |

### 3. Bayesian Optimization
Uses probabilistic model to guide search.

```python
# Using Optuna
import optuna

def objective(trial):
    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 15),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 50, 500),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
    }

    model = XGBClassifier(**params)
    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()
    return score

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100)

print(f"Best params: {study.best_params}")
print(f"Best score: {study.best_value:.4f}")
```

| Pros | Cons |
|------|------|
| Learns from previous trials | More complex setup |
| Efficient for expensive evaluations | Sequential (less parallelizable) |
| Handles complex search spaces | Overhead for cheap models |

### 4. Halving Search
Progressively eliminates poor candidates.

```python
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import HalvingGridSearchCV

halving_search = HalvingGridSearchCV(
    estimator=XGBClassifier(),
    param_grid=param_grid,
    factor=3,            # Elimination factor
    cv=5,
    scoring='f1',
    n_jobs=-1
)

halving_search.fit(X_train, y_train)
```

| Pros | Cons |
|------|------|
| Much faster than grid search | May eliminate good params early |
| Good for large search spaces | Requires enough data for stages |

## Comparison

| Method | Best For | Time Complexity |
|--------|----------|-----------------|
| Grid Search | Small grids, few params | O(n^k) |
| Random Search | Medium search spaces | O(n_iter) |
| Bayesian | Expensive models, complex spaces | O(n_trials) |
| Halving | Large grids, limited budget | O(log n) |

## Search Space Definition

### Continuous Parameters
```python
# Uniform
uniform(loc=0.01, scale=0.29)  # [0.01, 0.30]

# Log-uniform (for learning rates)
from scipy.stats import loguniform
loguniform(1e-5, 1e-1)

# Optuna
trial.suggest_float('lr', 1e-5, 1e-1, log=True)
```

### Integer Parameters
```python
# Scipy
randint(low=3, high=15)  # [3, 14]

# Optuna
trial.suggest_int('depth', 3, 15)
```

### Categorical Parameters
```python
# Grid
{'kernel': ['rbf', 'linear', 'poly']}

# Optuna
trial.suggest_categorical('kernel', ['rbf', 'linear', 'poly'])
```

## Early Stopping
Stop training when validation score stops improving:

```python
# XGBoost
model = XGBClassifier(
    n_estimators=1000,
    early_stopping_rounds=50
)
model.fit(
    X_train, y_train,
    eval_set=[(X_val, y_val)],
    verbose=False
)

# LightGBM
model = LGBMClassifier(
    n_estimators=1000,
    callbacks=[lgb.early_stopping(50)]
)
```

## Learning Rate Schedules
Reduce learning rate during training:

```python
# Keras
from tensorflow.keras.callbacks import ReduceLROnPlateau

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-6
)

model.fit(X, y, callbacks=[reduce_lr])
```

## Hyperparameter Importance

After tuning, analyze which parameters matter:

```python
# Optuna
optuna.visualization.plot_param_importances(study)

# From GridSearchCV results
import pandas as pd
results = pd.DataFrame(grid_search.cv_results_)
```

## Best Practices

1. **Start with random search**: Get a baseline, understand the space
2. **Use log scale for learning rates**: `loguniform(1e-5, 1e-1)`
3. **Set a budget**: Time or iteration limit
4. **Use cross-validation**: Avoid overfitting to validation set
5. **Consider nested CV**: For unbiased performance estimates
6. **Early stopping**: Especially for neural networks, boosting
7. **Document experiments**: Track what you tried

## Common Pitfalls

| Pitfall | Problem | Solution |
|---------|---------|----------|
| Tuning on test set | Data leakage | Use validation set only |
| Too fine grid | Wasted computation | Start coarse, refine |
| Ignoring interactions | Suboptimal results | Consider joint tuning |
| Overfitting to CV | Poor generalization | Hold out final test set |

## Frameworks

| Framework | Strengths |
|-----------|-----------|
| **Optuna** | Bayesian, pruning, visualization |
| **Ray Tune** | Distributed, schedulers |
| **Hyperopt** | Bayesian, TPE algorithm |
| **Weights & Biases** | Tracking, sweeps |
| **scikit-learn** | GridSearchCV, RandomizedSearchCV |

## Complete Example

```python
import optuna
from sklearn.model_selection import cross_val_score
from xgboost import XGBClassifier

def objective(trial):
    params = {
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
    }

    model = XGBClassifier(**params, use_label_encoder=False, eval_metric='logloss')
    scores = cross_val_score(model, X_train, y_train, cv=5, scoring='f1')
    return scores.mean()

# Run optimization
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=100, show_progress_bar=True)

# Train final model
best_model = XGBClassifier(**study.best_params)
best_model.fit(X_train, y_train)

# Evaluate on test set
print(f"Test F1: {f1_score(y_test, best_model.predict(X_test)):.4f}")
```

## Related Concepts
- [[12.08 Cross-Validation]] - Used during tuning
- [[12.07 Bias-Variance Tradeoff]] - Hyperparameters control complexity
- [[12.02 Overfitting and Underfitting]] - Tuning prevents both
- [[12.01 Ensemble Methods Bagging & Boosting techniques]]

## References
- Scikit-learn Tuning Documentation
- [Optuna Documentation](https://optuna.org/)
- "Random Search for Hyper-Parameter Optimization" (Bergstra & Bengio)
- "Algorithms for Hyper-Parameter Optimization" (Bergstra et al.)
