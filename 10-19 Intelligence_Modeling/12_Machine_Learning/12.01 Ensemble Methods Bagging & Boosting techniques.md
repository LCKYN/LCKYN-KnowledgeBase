---
id: 12.01
tags: [type/concept, status/seed, context/mle]
---

# Ensemble Methods: Bagging & Boosting

## Bagging (Bootstrap Aggregating)
Bootstrap Aggregating, also known as bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It decreases the variance and helps to avoid overfitting. It is usually applied to decision tree methods.

- Parallel training of independent models
- Reduces variance
- Example: Random Forest

## Boosting
Boosting is an ensemble modeling technique designed to create a strong classifier by combining multiple weak classifiers. The process involves building models sequentially, where each new model aims to correct the errors made by the previous ones.

- Sequential training where new models correct errors of previous ones
- Reduces bias and variance
- Examples: XGBoost, AdaBoost, Gradient Boosting

## Differences Between Bagging and Boosting

| Feature | Bagging | Boosting |
| :--- | :--- | :--- |
| **Function** | It combines multiple weak learners **independently in parallel**. | It combines weak learners **sequentially** to improve predictions. |
| **Goal** | Aims to **decrease variance** and avoid overfitting. | Aims to **decrease bias** and create a strong classifier. |
| **Method** | **Random sampling** with replacement (Bootstrap). | **Reweighting** misclassified data points. |
| **Dependency** | Models are **independent** of each other. | Each new model **depends on the previous one's errors**. |
| **Aggregation** | **Simple averaging** or majority voting. | **Weighted majority vote** based on model performance. |
| **Example** | Random Forest | AdaBoost, Gradient Boosting |
