---
id: 12.15
tags: [type/concept, status/evergreen, context/mle]
---

# SVM (Support Vector Machine)

## Overview
**Support Vector Machine (SVM)** is a supervised learning algorithm that finds the optimal **hyperplane** to separate classes with the **maximum margin**. It works for both classification (SVC) and regression (SVR), and can handle non-linear boundaries via the **kernel trick**.

## Key Concepts
- **Hyperplane**: Decision boundary that separates classes in feature space
- **Support Vectors**: Data points closest to the hyperplane — they define the margin
- **Margin**: Distance between the hyperplane and the nearest support vectors; SVM maximizes this
- **Hard Margin**: No misclassifications allowed (only works if data is linearly separable)
- **Soft Margin**: Allows some misclassifications via slack variables $\xi_i$, controlled by **C**
- **Kernel Trick**: Maps data to a higher-dimensional space without explicitly computing the transformation

## Objective Function

$$\min_{w,b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^{n} \xi_i$$

Subject to: $y_i(w \cdot x_i + b) \geq 1 - \xi_i, \quad \xi_i \geq 0$

- $C$ controls the **regularization** tradeoff: high C → fewer misclassifications, low C → wider margin

## Kernels

| Kernel | Formula | Best For |
|---|---|---|
| **Linear** | $K(x,z) = x \cdot z$ | High-dim, linearly separable data (text) |
| **RBF (Gaussian)** | $K(x,z) = \exp(-\gamma\|x-z\|^2)$ | General-purpose, non-linear boundaries |
| **Polynomial** | $K(x,z) = (x \cdot z + r)^d$ | Feature interactions up to degree $d$ |
| **Sigmoid** | $K(x,z) = \tanh(\gamma x \cdot z + r)$ | Rarely used; behaves like neural net |

> [!TIP] Kernel selection rule of thumb
> Start with **RBF** (default in scikit-learn). Use **Linear** when features >> samples (e.g., text/NLP). Use **Polynomial** when feature interactions matter.

## Key Hyperparameters

| Parameter | Effect | Tuning Guidance |
|---|---|---|
| **C** | Regularization strength (inverse) | Higher → tighter fit, risk overfitting |
| **gamma** (RBF/Poly) | Influence radius of each support vector | Higher → more complex boundary |
| **kernel** | Shape of decision boundary | Task-dependent (see table above) |
| **degree** (Poly) | Polynomial degree | Start at 2–3; higher = expensive |

> [!WARNING] Scaling is mandatory
> SVM is sensitive to feature scales. Always **standardize** (zero mean, unit variance) or **normalize** features before training.

## Comparison: SVM vs Other Classifiers

| Aspect | SVM | Logistic Regression | Random Forest |
|---|---|---|---|
| Decision boundary | Margin-based hyperplane | Probabilistic (sigmoid) | Ensemble of trees |
| Non-linearity | Kernel trick | Manual feature engineering | Inherent (splits) |
| Scalability | $O(n^2)$ to $O(n^3)$ | $O(n)$ | $O(n \log n)$ per tree |
| Interpretability | Low (kernel space) | High (coefficients) | Medium (feature importance) |
| Probability output | Not native (use Platt scaling) | Native | Native |

## Strengths & Limitations

| Strengths | Limitations |
|---|---|
| Effective in high-dimensional spaces | Slow on large datasets ($n > 10^5$) |
| Memory-efficient (only stores support vectors) | Poor scaling without feature normalization |
| Robust with proper regularization | No native probability estimates |
| Works well with clear margin of separation | Sensitive to C and gamma tuning |

## Practical Use Cases
- **Text classification** — Linear SVM excels on high-dim sparse TF-IDF features
- **Image classification** — RBF kernel on extracted features (pre-deep learning era)
- **Anomaly detection** — One-Class SVM for outlier detection
- **Bioinformatics** — Gene expression classification (high-dim, small sample)

## Related Concepts
- [[12_Machine_Learning_MOC]] - Parent category
- [[12.03 Supervised Learning]] - Context: classification technique
- [[12.04 Logistic Regression]] - Contrast: probabilistic vs margin-based
- [[12.07 Bias-Variance Tradeoff]] - C parameter controls this tradeoff
- [[12.09 Hyperparameter Tuning]] - Critical: C, gamma, kernel choice
- [[12.14 Imbalanced Classification]] - Use case: class weight adjustment
- [[12.02 Overfitting and Underfitting]] - C and gamma affect model complexity
