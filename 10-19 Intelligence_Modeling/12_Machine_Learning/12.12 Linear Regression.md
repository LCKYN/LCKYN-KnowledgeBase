---
id: 12.12
tags: [type/concept, status/evergreen, context/mle]
---

# Linear Regression

**Linear regression** models the relationship between a dependent variable and one or more independent variables by fitting a linear equation to observed data.

## Definition

**Simple Linear Regression** (one feature):
$$\hat{y} = w_0 + w_1 x$$

**Multiple Linear Regression** (multiple features):
$$\hat{y} = w_0 + w_1 x_1 + w_2 x_2 + \cdots + w_n x_n = \mathbf{w}^T \mathbf{x}$$

**Objective** (Minimize Mean Squared Error):
$$\mathcal{L} = \frac{1}{N} \sum_{i=1}^{N} (y_i - \hat{y}_i)^2$$

## Key Concepts

| Concept | Description |
|---------|-------------|
| Coefficients ($w$) | Weights learned from data |
| Intercept ($w_0$) | Bias term (y-intercept) |
| Residuals | Difference between actual and predicted |
| $R^2$ Score | Proportion of variance explained (0-1) |
| Normal Equation | Closed-form solution: $\mathbf{w} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y}$ |

## Assumptions

> [!WARNING] Violated Assumptions â†’ Poor Performance
> - **Linearity**: Relationship is linear
> - **Independence**: Observations are independent
> - **Homoscedasticity**: Constant variance of residuals
> - **Normality**: Residuals are normally distributed
> - **No multicollinearity**: Features are not highly correlated

## Regularization Variants

| Variant | Penalty | Use Case |
|---------|---------|----------|
| Ridge (L2) | $\lambda \sum w_i^2$ | Multicollinearity |
| Lasso (L1) | $\lambda \sum |w_i|$ | Feature selection |
| Elastic Net | L1 + L2 | Both benefits |

## Practical Use Cases

- **Price prediction**: Housing, stocks, products
- **Trend analysis**: Sales forecasting
- **Baseline model**: Simple benchmark before complex models
- **Feature importance**: Coefficient interpretation

## Evaluation Metrics

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| MSE | $\frac{1}{N}\sum(y-\hat{y})^2$ | Average squared error |
| RMSE | $\sqrt{MSE}$ | Same units as target |
| MAE | $\frac{1}{N}\sum|y-\hat{y}|$ | Average absolute error |
| $R^2$ | $1 - \frac{SS_{res}}{SS_{tot}}$ | Variance explained |

## Related Concepts

- [[12_Machine_Learning_MOC]] - parent category
- [[12.13 Polynomial Regression]] - extension for non-linear patterns
- [[12.04 Logistic Regression]] - classification variant using sigmoid
- [[12.07 Bias-Variance Tradeoff]] - linear regression has high bias
- [[13.09 Gradient Descent]] - optimization method for fitting
